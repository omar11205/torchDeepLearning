{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-13T13:08:20.882822Z",
     "start_time": "2024-09-13T13:08:20.876332Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set device and reproducibility",
   "id": "c11abba9e25ee2fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:34:05.857958Z",
     "start_time": "2024-09-13T15:34:05.854199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(101)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "d88c23d5532eae1b",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create a transformer instance: transform to tensor",
   "id": "1408a1b473bf6340"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T13:08:20.968867Z",
     "start_time": "2024-09-13T13:08:20.966434Z"
    }
   },
   "cell_type": "code",
   "source": "transform = transforms.ToTensor()",
   "id": "4a79e7925a2fcc70",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Import the MNIST Dataset and divide into train and test data",
   "id": "4703df59edc65839"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T13:08:20.999670Z",
     "start_time": "2024-09-13T13:08:20.970873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data = datasets.MNIST(root = \"train_images/MNIST\", train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root = \"train_images/MNIST\", train=False, download=True, transform=transform)"
   ],
   "id": "5de7e1837fea344f",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create the dataloaders with a small batch size (for CNN)",
   "id": "135937413d1e4070"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T13:08:21.036860Z",
     "start_time": "2024-09-13T13:08:21.034051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=10, shuffle=False)"
   ],
   "id": "a95341816a44235e",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Parameters of a convolutional layer\n",
    "Example:\n",
    "*conv1 = nn.Conv2d(1,6,3,1)* means:\n",
    "\n",
    "- 1, for one input channel (grayscale)\n",
    "- 6, output channels for feature extractions, the feature maps, the filters that the CNN will figure out\n",
    "- 3, kernel size for a 3 by 3 filter\n",
    "- 1, for the horizontal and vertical stride of the convolution\n",
    "\n",
    "### Simple training step with convolutional layers and polling layers\n",
    "- Input\n",
    "- Convolutional layer\n",
    "- Pooling layer\n",
    "- Flattening"
   ],
   "id": "79a282d4b7dbffca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:55:23.512081Z",
     "start_time": "2024-09-13T15:55:23.506083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define the convolutional layer\n",
    "conv1 = nn.Conv2d(1,6,3,1)\n",
    "\n",
    "for i, (X_train, Y_train) in enumerate(train_data):\n",
    "    break\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "\n",
    "# add an extra dimension to X_train to make the convolution (1 for batch)\n",
    "x = X_train.view(1,1,28,28)\n",
    "\n",
    "# perform a convolution \n",
    "out = conv1(x)\n",
    "print(\"Shape after the convolution:\", out.shape)\n",
    "\n",
    "# perform a pooling\n",
    "max_p = F.max_pool2d(out,2,2)\n",
    "print(\"Shape after the pooling\", max_p.shape)\n",
    "\n",
    "# How to feed this output of max_p to a fully connected input?\n",
    "x = max_p.view(-1, 6*13*13)\n",
    "print(\"Shape after the flattening\", x.shape)"
   ],
   "id": "4621bed59b1ba207",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: torch.Size([1, 28, 28])\n",
      "Shape after the convolution: torch.Size([1, 6, 26, 26])\n",
      "Shape after the pooling torch.Size([1, 6, 13, 13])\n",
      "Shape after the flattening torch.Size([1, 1014])\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### CNN Model\n",
    "Architecture of the neural network:\n",
    "\n",
    "CNN -> POL -> CNN -> POL -> FLAT -> FCL1 -> FCL2 -> FCL3"
   ],
   "id": "d118899224a655d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T17:54:59.433493Z",
     "start_time": "2024-09-13T17:54:59.429045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ConvolutionalNN(nn.Module):\n",
    "    def __init__(self, filter1, filter2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, filter1, 3, 1)  # Example: You can change this to any number of filters\n",
    "        self.conv2 = nn.Conv2d(6, filter2, 3, 1) # Example: You can change this to any number of filters\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Initialize fully connected layers with placeholder\n",
    "        self.fc1 = None\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "        # Dynamically compute the input size for the first fully connected layer\n",
    "        self._initialize_fc1()\n",
    "\n",
    "    def _initialize_fc1(self):\n",
    "        # Create a dummy tensor with the same size as an MNIST input\n",
    "        dummy_input = torch.zeros(1, 1, 28, 28)\n",
    "\n",
    "        # Pass the dummy input through the conv and pooling layers to calculate the flattened size\n",
    "        dummy_output = self.pool(F.relu(self.conv1(dummy_input)))\n",
    "        dummy_output = self.pool(F.relu(self.conv2(dummy_output)))\n",
    "        \n",
    "        # Calculate the flattened size\n",
    "        flattened_size = dummy_output.numel()  # Returns the total number of elements in the tensor\n",
    "        \n",
    "        # Now that we know the flattened size, we can initialize the first fully connected layer\n",
    "        self.fc1 = nn.Linear(flattened_size, 120)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = self.pool(X)  \n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = self.pool(X)  \n",
    "        X = X.view(X.size(0), -1)  # Flattening\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = self.fc3(X)\n",
    "        return F.log_softmax(X, dim=1)"
   ],
   "id": "ccbb413940077782",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T17:55:00.684819Z",
     "start_time": "2024-09-13T17:55:00.677480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = ConvolutionalNN(filter1=6, filter2=16).to(device)\n",
    "print(model)"
   ],
   "id": "d36631ab3e394ba8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalNN(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T17:55:23.041700Z",
     "start_time": "2024-09-13T17:55:23.038742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "d012136f98224180",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T17:57:06.348235Z",
     "start_time": "2024-09-13T17:55:23.598572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 5\n",
    "train_loss = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    trn_corr = 0\n",
    "    tst_corr = 0\n",
    "    \n",
    "    # train \n",
    "    for b, (X_trn, Y_trn) in enumerate(train_loader):\n",
    "        X_trn, Y_trn = X_trn.to(device), Y_trn.to(device)  # Ensure inputs and targets are on the same device\n",
    "        b += 1\n",
    "        \n",
    "        y_pred = model(X_trn)  # Fixed typo: use X_trn instead of X_train\n",
    "        loss = criterion(y_pred, Y_trn)\n",
    "        \n",
    "        predicted = torch.max(y_pred.data, 1)[1]\n",
    "        batch_corr = (predicted == Y_trn).sum()  # Corrected from y_pred to Y_trn\n",
    "        trn_corr += batch_corr\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if b % 600 == 0:\n",
    "            print(f'epoch: {i:2}  batch: {b:4} [{10*b:6}/60000]  loss: {loss.item():10.8f}  \\\n",
    "accuracy: {trn_corr.item()*100/(10*b):7.3f}%')\n",
    "    train_loss.append(loss.item())\n",
    "    train_correct.append(trn_corr)  # Use trn_corr instead of batch_corr for the entire epoch\n",
    "        \n",
    "    # test\n",
    "    with torch.no_grad():\n",
    "        for b, (X_test, Y_test) in enumerate(test_loader):\n",
    "            X_test, Y_test = X_test.to(device), Y_test.to(device)  # Ensure test inputs and targets are on the same device\n",
    "            y_val = model(X_test)\n",
    "            predicted = torch.max(y_val.data, 1)[1]\n",
    "            tst_corr += (predicted == Y_test).sum()\n",
    "        \n",
    "    loss = criterion(y_val, Y_test)\n",
    "    test_losses.append(loss.item())\n",
    "    test_correct.append(tst_corr)\n",
    "        \n",
    "current_time = time.time()\n",
    "total = current_time - start_time\n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds')"
   ],
   "id": "317b944396914dde",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  batch:  600 [  6000/60000]  loss: 0.50230491  accuracy:  73.650%\n",
      "epoch:  0  batch: 1200 [ 12000/60000]  loss: 0.32116914  accuracy:  83.242%\n",
      "epoch:  0  batch: 1800 [ 18000/60000]  loss: 0.38754264  accuracy:  87.172%\n",
      "epoch:  0  batch: 2400 [ 24000/60000]  loss: 0.35554084  accuracy:  89.242%\n",
      "epoch:  0  batch: 3000 [ 30000/60000]  loss: 0.08998176  accuracy:  90.667%\n",
      "epoch:  0  batch: 3600 [ 36000/60000]  loss: 0.08671220  accuracy:  91.642%\n",
      "epoch:  0  batch: 4200 [ 42000/60000]  loss: 0.52407354  accuracy:  92.362%\n",
      "epoch:  0  batch: 4800 [ 48000/60000]  loss: 0.41157693  accuracy:  92.902%\n",
      "epoch:  0  batch: 5400 [ 54000/60000]  loss: 0.16420719  accuracy:  93.381%\n",
      "epoch:  0  batch: 6000 [ 60000/60000]  loss: 0.00142492  accuracy:  93.795%\n",
      "epoch:  0  batch:  600 [  6000/60000]  loss: 0.00271948  accuracy:  97.783%\n",
      "epoch:  0  batch: 1200 [ 12000/60000]  loss: 0.04339140  accuracy:  97.542%\n",
      "epoch:  0  batch: 1800 [ 18000/60000]  loss: 0.00172344  accuracy:  97.656%\n",
      "epoch:  0  batch: 2400 [ 24000/60000]  loss: 0.05630564  accuracy:  97.737%\n",
      "epoch:  0  batch: 3000 [ 30000/60000]  loss: 0.08324395  accuracy:  97.767%\n",
      "epoch:  0  batch: 3600 [ 36000/60000]  loss: 0.00094617  accuracy:  97.761%\n",
      "epoch:  0  batch: 4200 [ 42000/60000]  loss: 0.00246957  accuracy:  97.798%\n",
      "epoch:  0  batch: 4800 [ 48000/60000]  loss: 0.01387783  accuracy:  97.821%\n",
      "epoch:  0  batch: 5400 [ 54000/60000]  loss: 0.14104471  accuracy:  97.881%\n",
      "epoch:  0  batch: 6000 [ 60000/60000]  loss: 0.00130642  accuracy:  97.928%\n",
      "epoch:  0  batch:  600 [  6000/60000]  loss: 0.00014600  accuracy:  98.800%\n",
      "epoch:  0  batch: 1200 [ 12000/60000]  loss: 0.00110018  accuracy:  98.683%\n",
      "epoch:  0  batch: 1800 [ 18000/60000]  loss: 0.00026190  accuracy:  98.639%\n",
      "epoch:  0  batch: 2400 [ 24000/60000]  loss: 0.00013283  accuracy:  98.583%\n",
      "epoch:  0  batch: 3000 [ 30000/60000]  loss: 0.00025948  accuracy:  98.563%\n",
      "epoch:  0  batch: 3600 [ 36000/60000]  loss: 0.00043897  accuracy:  98.567%\n",
      "epoch:  0  batch: 4200 [ 42000/60000]  loss: 0.00408358  accuracy:  98.560%\n",
      "epoch:  0  batch: 4800 [ 48000/60000]  loss: 0.00026421  accuracy:  98.581%\n",
      "epoch:  0  batch: 5400 [ 54000/60000]  loss: 0.05496340  accuracy:  98.578%\n",
      "epoch:  0  batch: 6000 [ 60000/60000]  loss: 0.01021575  accuracy:  98.575%\n",
      "epoch:  0  batch:  600 [  6000/60000]  loss: 0.12358032  accuracy:  98.917%\n",
      "epoch:  0  batch: 1200 [ 12000/60000]  loss: 0.00056413  accuracy:  98.908%\n",
      "epoch:  0  batch: 1800 [ 18000/60000]  loss: 0.00133554  accuracy:  98.867%\n",
      "epoch:  0  batch: 2400 [ 24000/60000]  loss: 0.00106272  accuracy:  98.817%\n",
      "epoch:  0  batch: 3000 [ 30000/60000]  loss: 0.00055177  accuracy:  98.857%\n",
      "epoch:  0  batch: 3600 [ 36000/60000]  loss: 0.00026354  accuracy:  98.844%\n",
      "epoch:  0  batch: 4200 [ 42000/60000]  loss: 0.00099890  accuracy:  98.836%\n",
      "epoch:  0  batch: 4800 [ 48000/60000]  loss: 0.02011451  accuracy:  98.808%\n",
      "epoch:  0  batch: 5400 [ 54000/60000]  loss: 0.00165393  accuracy:  98.820%\n",
      "epoch:  0  batch: 6000 [ 60000/60000]  loss: 0.00178279  accuracy:  98.830%\n",
      "epoch:  0  batch:  600 [  6000/60000]  loss: 0.06241726  accuracy:  98.800%\n",
      "epoch:  0  batch: 1200 [ 12000/60000]  loss: 0.05316677  accuracy:  98.850%\n",
      "epoch:  0  batch: 1800 [ 18000/60000]  loss: 0.00008269  accuracy:  98.989%\n",
      "epoch:  0  batch: 2400 [ 24000/60000]  loss: 0.00035536  accuracy:  99.021%\n",
      "epoch:  0  batch: 3000 [ 30000/60000]  loss: 0.00047456  accuracy:  99.030%\n",
      "epoch:  0  batch: 3600 [ 36000/60000]  loss: 0.00000262  accuracy:  99.050%\n",
      "epoch:  0  batch: 4200 [ 42000/60000]  loss: 0.00005090  accuracy:  99.033%\n",
      "epoch:  0  batch: 4800 [ 48000/60000]  loss: 0.16674212  accuracy:  99.015%\n",
      "epoch:  0  batch: 5400 [ 54000/60000]  loss: 0.00045343  accuracy:  99.019%\n",
      "epoch:  0  batch: 6000 [ 60000/60000]  loss: 0.02505518  accuracy:  99.025%\n",
      "\n",
      "Duration: 103 seconds\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7879d27036d2d8f6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
