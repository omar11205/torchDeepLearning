{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Prediction of New York City Taxi Fares\n",
    "In this section we will transform the dataset in order to create new useful datetime and distance features, with this new features we will try to predict with regression the fare amount of the taxi rides taking with the rest of the features less the fare class. In the second part we will try to predict the fare class using embeddings and PyTorch embedding layers for the zipcodes.\n",
    "\n",
    "Description of the dataset\n",
    "\n",
    "Features:\n",
    " \n",
    "- pickup_datetime - timestamp value indicating when the taxi ride started.\n",
    "- pickup_longitude - float for longitude coordinate of where the taxi ride started.\n",
    "- pickup_latitude - float for latitude coordinate of where the taxi ride started.\n",
    "- dropoff_longitude - float for longitude coordinate of where the taxi ride ended.\n",
    "- dropoff_latitude - float for latitude coordinate of where the taxi ride ended.\n",
    "- passenger_count - integer indicating the number of passengers in the taxi ride.\n",
    "\n",
    "Target\n",
    "- fare_amount: float dollar amount of the cost of the taxi ride. This value is only in the training set; this is what you are predicting in the test set and it is required in your submission CSV.\n",
    "\n",
    "### Get the traveled distance with the coordinates\n",
    "\n",
    "A Regression approach that correlates the traveled distance with the fare amount per ride needs a traveled distance calculation with the coordinates provided. For this reason a new feature could be calculated with the Haversine formula that calculates the distance on a sphere between two sets of GPS coordinates. With this, we reduce the complexity of the travel with a straight line. \n",
    "\n",
    "The distance formula works out to\n",
    "\n",
    "$${\\displaystyle d=2r\\arcsin \\left({\\sqrt {\\sin ^{2}\\left({\\frac {\\varphi _{2}-\\varphi _{1}}{2}}\\right)+\\cos(\\varphi _{1})\\:\\cos(\\varphi _{2})\\:\\sin ^{2}\\left({\\frac {\\lambda _{2}-\\lambda _{1}}{2}}\\right)}}\\right)}$$\n",
    "\n",
    "where\n",
    "\n",
    "$\\begin{split} r&: \\textrm {radius of the sphere (Earth's radius averages 6371 km)}\\\\\n",
    "\\varphi_1, \\varphi_2&: \\textrm {latitudes of point 1 and point 2}\\\\\n",
    "\\lambda_1, \\lambda_2&: \\textrm {longitudes of point 1 and point 2}\\end{split}$"
   ],
   "id": "4ce256f93be403d0"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-14T14:30:47.895719Z",
     "start_time": "2024-08-14T14:30:46.300419Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import seaborn as sns\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setting device, reproducibility and Early Stopping ",
   "id": "8020f9fb538928b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T14:30:48.787423Z",
     "start_time": "2024-08-14T14:30:48.773130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "has_mps = torch.backends.mps.is_built()\n",
    "# device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Defining an early stopping class for PyTorch\n",
    "import copy\n",
    "class EarlyStopping:\n",
    "  def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
    "    self.patience = patience\n",
    "    self.min_delta = min_delta\n",
    "    self.restore_best_weights = restore_best_weights\n",
    "    self.best_model = None\n",
    "    self.best_loss = None\n",
    "    self.patience_counter = 0\n",
    "    self.status = \"\"\n",
    "\n",
    "  def __call__(self, model, val_loss):\n",
    "    if self.best_loss is None:\n",
    "      self.best_loss = val_loss\n",
    "      self.best_model = copy.deepcopy(model.state_dict())\n",
    "    elif self.best_loss - val_loss >= self.min_delta:\n",
    "      self.best_model = copy.deepcopy(model.state_dict())\n",
    "      self.best_loss = val_loss\n",
    "      self.status = f\"Improvement!!!, actual counter {self.patience_counter}\"\n",
    "      self.patience_counter = 0\n",
    "    else:\n",
    "      self.patience_counter += 1\n",
    "      self.status = f\"NO improvement in the last {self.patience_counter} epochs\"\n",
    "      if self.patience_counter >= self.patience:\n",
    "        self.status = f\"Early stopping triggered after {self.patience_counter} epochs.\"\n",
    "        if self.restore_best_weights:\n",
    "          model.load_state_dict(self.best_model)\n",
    "        return True\n",
    "    return False"
   ],
   "id": "cf1a78a3727a15db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import dataset and drop NA values",
   "id": "722b8c4a572dd5b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T14:30:51.672051Z",
     "start_time": "2024-08-14T14:30:51.569907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# read the MPG dataset\n",
    "df = pd.read_csv(\"data/NYCTaxiFares.csv\", na_values=[\"NA\", \"?\"])\n",
    "\n",
    "# check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)"
   ],
   "id": "1a36c0e550f264a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_datetime      0\n",
      "fare_amount          0\n",
      "fare_class           0\n",
      "pickup_longitude     0\n",
      "pickup_latitude      0\n",
      "dropoff_longitude    0\n",
      "dropoff_latitude     0\n",
      "passenger_count      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Estimating the traveled distance per ride with the Haversine Formula",
   "id": "ca77745b37d7fe8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T14:30:55.182231Z",
     "start_time": "2024-08-14T14:30:55.170364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function to calculate the distance of the travel\n",
    "def haversine_distance(df, lat1, lon1, lat2, lon2):\n",
    "    \n",
    "    # average radius of the Earth in (km)\n",
    "    r = 6371\n",
    "    \n",
    "    phi1 = np.radians(df[lat1])\n",
    "    phi2 = np.radians(df[lat2])\n",
    "    delta_phi = np.radians(df[lat2] - df[lat1])\n",
    "    delta_lambda = np.radians(df[lon2] - df[lon1])\n",
    "    \n",
    "    a = np.sin(delta_phi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    d = (r * c)\n",
    "    \n",
    "    return d\n",
    "\n",
    "# append a 'dist_km' new feature in the dataframe\n",
    "df['dist_km'] = haversine_distance(df, 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude')"
   ],
   "id": "9bc75a71f63a4714",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Descriptive analytics\n",
    "Check and remove outliers"
   ],
   "id": "6dfc5a518d5a3e5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df['fare_amount'].describe()",
   "id": "acbf5da6314a47da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plotting the distribution of fare_amount\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['fare_amount'], bins=50, kde=True)\n",
    "plt.title('Distribution of Fare Amount')\n",
    "plt.xlabel('Fare Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ],
   "id": "ffdf1e38d61b364a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plotting the scatter plot of fare_amount vs dist_km\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=df['dist_km'], y=df['fare_amount'])\n",
    "plt.title('Fare Amount vs Distance (km)')\n",
    "plt.xlabel('Distance (km)')\n",
    "plt.ylabel('Fare Amount')\n",
    "plt.show()"
   ],
   "id": "6e58a7900c418bf0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = df[(df['fare_amount'] != 49.57) & (df['fare_amount'] != 45.00)]\n",
    "# Plotting the scatter plot of fare_amount vs dist_km\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=df['dist_km'], y=df['fare_amount'])\n",
    "plt.title('Fare Amount vs Distance (km)')\n",
    "plt.xlabel('Distance (km)')\n",
    "plt.ylabel('Fare Amount')\n",
    "plt.show()"
   ],
   "id": "561b2a9ac63ff6f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plotting the distribution of fare_amount\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['fare_amount'], bins=50, kde=True)\n",
    "plt.title('Distribution of Fare Amount')\n",
    "plt.xlabel('Fare Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ],
   "id": "4e4ab1b557c5b9cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df['fare_amount'].describe()",
   "id": "f33c39e4b705a9aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Time datatypes transformations and charts\n",
    "To work with the special pandas dtype timestamps the to_datetime method can be used."
   ],
   "id": "cea38f6614813d27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T14:31:03.773812Z",
     "start_time": "2024-08-14T14:31:02.800259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# actual dtypes\n",
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])"
   ],
   "id": "45b8c4bf8671f830",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Correcting pickup_datetime due to daylight savings time (April) There is a 4 hour difference between the value in the dataframe and the real NYC time. Eastern Day Time.",
   "id": "b76a840b247ce3ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T14:31:05.356823Z",
     "start_time": "2024-08-14T14:31:05.343906Z"
    }
   },
   "cell_type": "code",
   "source": "df['EDTdate'] = df['pickup_datetime'] - pd.Timedelta(hours=4)",
   "id": "ae7f36a02e8631fb",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Extract new features from the time series",
   "id": "8ae6b42e6e3fdaa9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T14:31:07.945500Z",
     "start_time": "2024-08-14T14:31:07.495793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['Hour'] = df['EDTdate'].dt.hour\n",
    "df['AMorPM'] = np.where(df['Hour']<12, 'am', 'pm')\n",
    "df['Weekday'] = df['EDTdate'].dt.strftime(\"%a\")"
   ],
   "id": "c970a91820fa9d24",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filter the dataframe to include only rows of Night Hours\n",
    "df_filtered = df[df['Hour'].isin([23,0,1,2,3,4])]\n",
    "\n",
    "# Plotting the scatter plot of fare_amount vs dist_km colored by Night Hours\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=df_filtered['dist_km'], y=df_filtered['fare_amount'], hue=df_filtered['Hour'].astype(str), alpha=0.6)\n",
    "plt.title('Fare Amount vs Distance (km) Night Hours categories')\n",
    "plt.xlabel('Distance (km)')\n",
    "plt.ylabel('Fare Amount')\n",
    "plt.legend(title='Hour')\n",
    "plt.show()"
   ],
   "id": "75e3a01c360a699b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a FacetGrid to plot histograms for each 'Hour' category\n",
    "g = sns.FacetGrid(df, col='Hour', col_wrap=4, height=4, aspect=1.5, sharex=False, sharey=False)\n",
    "g.map(sns.histplot, 'fare_amount', kde=False, bins=30, color='blue', alpha=0.7)\n",
    "\n",
    "# Add titles and labels\n",
    "g.set_titles(col_template='Hour: {col_name}')\n",
    "g.set_axis_labels('Fare Amount', 'Count')\n",
    "g.fig.suptitle('Histograms of Fare Amount by Hour', y=1.02)\n",
    "\n",
    "plt.show()"
   ],
   "id": "dbd28ef60d990581",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Managing categorical and continuous values in Pandas\n",
    "Defining arbitrary continuous categories, arbitrary continuous columns and the target feature"
   ],
   "id": "a3b168ca538b778a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# set a list of categorical fields\n",
    "cat_cols = ['Hour', 'AMorPM', 'Weekday']\n",
    "\n",
    "# set a list of continuous fields\n",
    "cont_cols = ['passenger_count', 'dist_km']\n",
    "\n",
    "# this column contains the labels (Y)\n",
    "y_col = ['fare_amount']\n",
    "\n",
    "# transform to pandas categorical variables\n",
    "for cat in cat_cols:\n",
    "    df[cat] = df[cat].astype('category')"
   ],
   "id": "2064caece9518c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Numpy arrays for training datasets",
   "id": "e91949528761e251"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# create categorical codes matrix with list comprehension\n",
    "cats = np.stack([df[col].cat.codes.values for col in cat_cols], axis=1)\n",
    "\n",
    "# create continuous matrix with list comprehension\n",
    "conts = np.stack([df[col].values for col in cont_cols], axis=1)\n",
    "\n",
    "# create labels numpy (y)\n",
    "y = df[y_col].values.reshape(-1, 1)"
   ],
   "id": "57b236f89632bba3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Creating lists of Categorical sizes for PyTorch Embedding Layers",
   "id": "98a9bfaa2b01cc53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# categorical sizes list\n",
    "cat_sizes = [len(df[col].cat.categories) for col in cat_cols]\n",
    "\n",
    "# embedding sizes list (divide the number of unique entries in each column by two, if the result is greater than 50 select 50)\n",
    "emb_sizes = [(size, min(50,(size+1)//2)) for size in cat_sizes]"
   ],
   "id": "6c7d37a9f6e6fcdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tabular model definition",
   "id": "649be95c794d67bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class TabularModel(nn.Module):\n",
    "    def __init__(self, emb_sizes_s, n_cont, out_sizes, layers, p=0.5):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in emb_sizes_s])\n",
    "        self.emb_drop = nn.Dropout(p)\n",
    "        self.batch_norm_cont = nn.BatchNorm1d(n_cont)\n",
    "        \n",
    "        layer_list = []\n",
    "        n_emb = sum([nf for ni, nf in emb_sizes_s])\n",
    "        n_in = n_emb + n_cont\n",
    "        for i in layers:\n",
    "            layer_list.append(nn.Linear(n_in, i))\n",
    "            layer_list.append(nn.ReLU(inplace=True))\n",
    "            layer_list.append(nn.BatchNorm1d(i))\n",
    "            layer_list.append(nn.Dropout(p))\n",
    "            n_in = i\n",
    "            \n",
    "        layer_list.append(nn.Linear(layers[-1], out_sizes))\n",
    "        \"\"\"In Python, the asterisk (*) operator is used for unpacking a list or a tuple. \"\"\"\n",
    "        self.layers = nn.Sequential(*layer_list)\n",
    "    \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        embeddings = []\n",
    "        for i, e in enumerate(self.embeds):\n",
    "            embeddings.append(e(x_cat[:, i]))\n",
    "        \n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        \n",
    "        x_cont = self.batch_norm_cont(x_cont)\n",
    "        x = torch.cat([x, x_cont], 1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ],
   "id": "55c9c6cb9878f41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Function to split not in 4 (trains x 2, tests x 2) but in 6 (trains x 3, tests x 3) the selected dataset for automated learning using array slices",
   "id": "958619d820b8a721"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_train_test(categoricals, continuous, y_train, test_size=0.2):\n",
    "    # Ensure the input arrays have the same number of rows\n",
    "    assert categoricals.shape[0] == continuous.shape[0] == y_train.shape[0], \"Input arrays must have the same number of rows\"\n",
    "\n",
    "    # Combine the data into a single array for splitting\n",
    "    combined = np.hstack((categoricals, continuous, y_train))\n",
    "\n",
    "    # Split the combined data into train and test sets\n",
    "    train_data, test_data = train_test_split(combined, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Determine the number of categorical and continuous columns\n",
    "    n_cat_cols = categoricals.shape[1]\n",
    "    n_cont_cols = continuous.shape[1]\n",
    "\n",
    "    # Separate the train and test data back into categorical, continuous, and target tensors\n",
    "\n",
    "    # selects all rows and the first n_cat_cols columns (categorical features).\n",
    "    cat_train = torch.tensor(train_data[:, :n_cat_cols], dtype=torch.int64).to(device)\n",
    "\n",
    "    # selects all rows and the columns from n_cat_cols to n_cat_cols + n_cont_cols (continuous features).\n",
    "    con_train = torch.tensor(train_data[:, n_cat_cols:n_cat_cols + n_cont_cols], dtype=torch.float32).to(device)\n",
    "\n",
    "    # selects all rows and the last column (target labels).\n",
    "    y_train = torch.tensor(train_data[:, -1], dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    cat_test = torch.tensor(test_data[:, :n_cat_cols], dtype=torch.int64).to(device)\n",
    "    con_test = torch.tensor(test_data[:, n_cat_cols:n_cat_cols + n_cont_cols], dtype=torch.float32).to(device)\n",
    "    y_test = torch.tensor(test_data[:, -1], dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    return cat_train, con_train, y_train, cat_test, con_test, y_test\n",
    "\n",
    "cat_train, con_train, y_train, cat_test, con_test, y_test = get_train_test(cats, conts, y)"
   ],
   "id": "45462b6a47fe77f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Defining training instances\n",
    "The training will be made with batches of 2000 randomly chosen elements using TensorDatasets and DataLoaders and 100 epochs for batch  "
   ],
   "id": "56f8207936bf05c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# model instance (output_size = 1 for regression task)\n",
    "model = TabularModel(emb_sizes, conts.shape[1], 1, [200, 150, 100], p=0.4).to(device)\n",
    "\n",
    "# criterion\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# TensorDatasets\n",
    "train_dataset = TensorDataset(cat_train, con_train, y_train)\n",
    "test_dataset = TensorDataset(cat_test, con_test, y_test)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# early stop \n",
    "early_stopping = EarlyStopping(patience=100)"
   ],
   "id": "a68fe92a4aeb33eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exploration Training loop\n",
    "The next is a training loop to determine the possible initial configuration for further trainings, in the next sections are developed training loops to fine tune hyperparameters "
   ],
   "id": "287dac0c1c0a43e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "epochs = 100\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "\n",
    "    for cat_batch, con_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(cat_batch, con_batch).flatten()\n",
    "        loss = torch.sqrt(criterion(y_pred, y_batch.flatten()))  # Use y_batch.flatten() here\n",
    "        epoch_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_losses.append(np.mean(epoch_losses))\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_val_loss = []\n",
    "        for cat_batch, con_batch, y_batch in test_loader:\n",
    "            y_val = model(cat_batch, con_batch).flatten()\n",
    "            val_loss = torch.sqrt(criterion(y_val, y_batch.flatten()))  # Use y_batch.flatten() here\n",
    "            epoch_val_loss.append(val_loss.item())\n",
    "\n",
    "        val_losses.append(np.mean(epoch_val_loss))\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == epochs - 1:\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Validation Loss: {val_losses[-1]:.4f}, Early S: {early_stopping.status}')\n",
    "\n",
    "    # Check early stopping\n",
    "    if early_stopping(model, val_losses[-1]):  # Pass the latest validation loss to early stopping\n",
    "        print(early_stopping.status)\n",
    "        break\n",
    "\n",
    "print(f'Training completed in {time.time() - start_time:.2f} seconds')"
   ],
   "id": "c4ff1dcded31621e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f'{\"PREDICTED\":>12} {\"ACTUAL\":>8} {\"DIFF\":>8}')\n",
    "for i in range(50):\n",
    "    diff = np.abs(y_val[i].item()-y_test[i].item())\n",
    "    print(f'{i+1:2}. {y_val[i].item():8.4f} {y_test[i].item():8.4f} {diff:8.4f}')"
   ],
   "id": "29549fe9e694e8e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to reset weights and biases\n",
    "def reset_weights(m):\n",
    "    if hasattr(m, 'reset_parameters'):\n",
    "        m.reset_parameters()\n",
    "\n",
    "# Apply the reset function to each layer\n",
    "model.apply(reset_weights)"
   ],
   "id": "c535933552b0210c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3ad69e6f25648824",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
