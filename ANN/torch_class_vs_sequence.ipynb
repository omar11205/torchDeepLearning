{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Sequences vs Classes in PyTorch\n",
    "Alternatively to nn.Sequential there is another commonly method of model definition an that is using a model class."
   ],
   "id": "5663757d5c8424ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Defining neural networks as sequences\n",
    "In Pytorch the representation of a neural networks can be made as a sequence of layers. Each layer performs a specific computation on the input data and passes the transformed output to the next layer. \n",
    "\n",
    "As example:\n",
    "\n",
    "```\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "```\n",
    "\n",
    "This neural network have 3 dense layers interspersed with ReLU activation functions. The final layer utilizes the LogSoftMax activation commonly used in classification tasks."
   ],
   "id": "ffeb39166afb1414"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Defining Neural Networks With classes\n",
    "nn.Sequential approach is intuitive and convinient in many cases but it can become limiting when we need more flexibility in the network achitecture. Defining neural networks as classes allows us to create custom architectures with complex behaviours and shareable components.\n",
    "\n",
    "To define a neural network as a class, we typically subclass the nn.Module provided by PyTorch. This base class offers essential functionality for organizing the network's parameters and handling computations during forward and backward passes.\n",
    "\n",
    "```\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.log_softmax(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the CustomNetwork\n",
    "model = CustomNetwork()        \n",
    "```\n",
    "In this example, we define a class called CustomNetwork that inherits from nn.Module. Inside the class, we describe the network's layers as attributes. The forward method specifies how the input flows through these layers during the forward pass.\n",
    "\n",
    "By defining neural networks as classes, we can create more complex architectures, leverage conditional logic within the network, and encapsulate reusable components. This flexibility becomes particularly useful as we delve into advanced topics throughout the course.\n",
    "\n"
   ],
   "id": "6eb55ebf64dcdb98"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Class Example\n",
   "id": "b7d202b1eacb3363"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:39:41.898316Z",
     "start_time": "2024-07-27T15:39:41.890329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable"
   ],
   "id": "4be428f68bd452a0",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:39:43.851215Z",
     "start_time": "2024-07-27T15:39:43.842998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Define each of the layers\n",
    "        self.layer1 = nn.Linear(input_dim, 50)\n",
    "        self.layer2 = nn.Linear(50, 25)\n",
    "        self.layer3 = nn.Linear(25, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # pass the input through the layers\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "    "
   ],
   "id": "6277c04d7987706",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:43:48.332215Z",
     "start_time": "2024-07-27T15:43:48.299690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fill_na_with_median(df):\n",
    "    \"\"\"\n",
    "    Replace all na values in numeric fields with median \n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['float32', 'int32', 'int64', 'float64', 'long']:\n",
    "            median = df[col].median()\n",
    "            df[col].fillna(median, inplace=True)\n",
    "    return df\n",
    "\n",
    "def load_auto_mpg():\n",
    "    df = pd.read_csv(\"data/auto-mpg.csv\", na_values=[\"NA\", \"?\"])\n",
    "    df = fill_na_with_median(df)\n",
    "    return df\n",
    "\n",
    "df = load_auto_mpg()\n",
    "df.info()"
   ],
   "id": "eb0817256bd9bb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 398 entries, 0 to 397\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   mpg           398 non-null    float64\n",
      " 1   cylinders     398 non-null    int64  \n",
      " 2   displacement  398 non-null    float64\n",
      " 3   horsepower    398 non-null    float64\n",
      " 4   weight        398 non-null    int64  \n",
      " 5   acceleration  398 non-null    float64\n",
      " 6   year          398 non-null    int64  \n",
      " 7   origin        398 non-null    int64  \n",
      " 8   name          398 non-null    object \n",
      "dtypes: float64(4), int64(4), object(1)\n",
      "memory usage: 28.1+ KB\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:45:21.098790Z",
     "start_time": "2024-07-27T15:45:21.089987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# creating tensors for regression model\n",
    "def regression_tensors(df, feature_columns, target_column, default_device=\"cpu\"):\n",
    "    x = torch.tensor(df[feature_columns].values, device=default_device, dtype=torch.float32)\n",
    "    y = torch.tensor(df[target_column].values, device = default_device, dtype=torch.float32)\n",
    "    return x, y\n",
    "\n",
    "x, y = regression_tensors(df, feature_columns=[\n",
    "            \"cylinders\",\n",
    "            \"displacement\",\n",
    "            \"horsepower\",\n",
    "            \"weight\",\n",
    "            \"acceleration\",\n",
    "            \"year\",\n",
    "            \"origin\",\n",
    "        ], target_column=\"mpg\")"
   ],
   "id": "bce48283393841b9",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:45:21.744498Z",
     "start_time": "2024-07-27T15:45:21.735704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# setting the model for regression task \n",
    "# model instantiation\n",
    "device = \"cpu\"\n",
    "model = Net(x.shape[1], 1).to(device)\n",
    "\n",
    "# define the loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.00001)"
   ],
   "id": "272e9df2326bf53b",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:45:41.444682Z",
     "start_time": "2024-07-27T15:45:40.586817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# training loop \n",
    "for epoch in range(1000):\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # forward pass\n",
    "    y_pred = model.forward(x).flatten()\n",
    "    \n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item():.4f}\")"
   ],
   "id": "1910ee4f499b6a6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 624.1686\n",
      "Epoch: 5, Loss: 623.5555\n",
      "Epoch: 10, Loss: 622.9426\n",
      "Epoch: 15, Loss: 622.3298\n",
      "Epoch: 20, Loss: 621.7174\n",
      "Epoch: 25, Loss: 621.1052\n",
      "Epoch: 30, Loss: 620.4931\n",
      "Epoch: 35, Loss: 619.8813\n",
      "Epoch: 40, Loss: 619.2697\n",
      "Epoch: 45, Loss: 618.6582\n",
      "Epoch: 50, Loss: 618.0469\n",
      "Epoch: 55, Loss: 617.4358\n",
      "Epoch: 60, Loss: 616.8248\n",
      "Epoch: 65, Loss: 616.2139\n",
      "Epoch: 70, Loss: 615.6031\n",
      "Epoch: 75, Loss: 614.9925\n",
      "Epoch: 80, Loss: 614.3820\n",
      "Epoch: 85, Loss: 613.7715\n",
      "Epoch: 90, Loss: 613.1611\n",
      "Epoch: 95, Loss: 612.5509\n",
      "Epoch: 100, Loss: 611.9407\n",
      "Epoch: 105, Loss: 611.3306\n",
      "Epoch: 110, Loss: 610.7205\n",
      "Epoch: 115, Loss: 610.1104\n",
      "Epoch: 120, Loss: 609.5004\n",
      "Epoch: 125, Loss: 608.8904\n",
      "Epoch: 130, Loss: 608.2804\n",
      "Epoch: 135, Loss: 607.6705\n",
      "Epoch: 140, Loss: 607.0604\n",
      "Epoch: 145, Loss: 606.4504\n",
      "Epoch: 150, Loss: 605.8405\n",
      "Epoch: 155, Loss: 605.2303\n",
      "Epoch: 160, Loss: 604.6202\n",
      "Epoch: 165, Loss: 604.0101\n",
      "Epoch: 170, Loss: 603.3999\n",
      "Epoch: 175, Loss: 602.7897\n",
      "Epoch: 180, Loss: 602.1793\n",
      "Epoch: 185, Loss: 601.5688\n",
      "Epoch: 190, Loss: 600.9583\n",
      "Epoch: 195, Loss: 600.3477\n",
      "Epoch: 200, Loss: 599.7369\n",
      "Epoch: 205, Loss: 599.1261\n",
      "Epoch: 210, Loss: 598.5151\n",
      "Epoch: 215, Loss: 597.9040\n",
      "Epoch: 220, Loss: 597.2927\n",
      "Epoch: 225, Loss: 596.6813\n",
      "Epoch: 230, Loss: 596.0697\n",
      "Epoch: 235, Loss: 595.4580\n",
      "Epoch: 240, Loss: 594.8460\n",
      "Epoch: 245, Loss: 594.2339\n",
      "Epoch: 250, Loss: 593.6215\n",
      "Epoch: 255, Loss: 593.0091\n",
      "Epoch: 260, Loss: 592.3964\n",
      "Epoch: 265, Loss: 591.7834\n",
      "Epoch: 270, Loss: 591.1703\n",
      "Epoch: 275, Loss: 590.5568\n",
      "Epoch: 280, Loss: 589.9433\n",
      "Epoch: 285, Loss: 589.3294\n",
      "Epoch: 290, Loss: 588.7153\n",
      "Epoch: 295, Loss: 588.1009\n",
      "Epoch: 300, Loss: 587.4862\n",
      "Epoch: 305, Loss: 586.8712\n",
      "Epoch: 310, Loss: 586.2560\n",
      "Epoch: 315, Loss: 585.6406\n",
      "Epoch: 320, Loss: 585.0247\n",
      "Epoch: 325, Loss: 584.4086\n",
      "Epoch: 330, Loss: 583.7922\n",
      "Epoch: 335, Loss: 583.1754\n",
      "Epoch: 340, Loss: 582.5583\n",
      "Epoch: 345, Loss: 581.9409\n",
      "Epoch: 350, Loss: 581.3231\n",
      "Epoch: 355, Loss: 580.7050\n",
      "Epoch: 360, Loss: 580.0865\n",
      "Epoch: 365, Loss: 579.4677\n",
      "Epoch: 370, Loss: 578.8484\n",
      "Epoch: 375, Loss: 578.2288\n",
      "Epoch: 380, Loss: 577.6089\n",
      "Epoch: 385, Loss: 576.9885\n",
      "Epoch: 390, Loss: 576.3677\n",
      "Epoch: 395, Loss: 575.7465\n",
      "Epoch: 400, Loss: 575.1249\n",
      "Epoch: 405, Loss: 574.5030\n",
      "Epoch: 410, Loss: 573.8805\n",
      "Epoch: 415, Loss: 573.2576\n",
      "Epoch: 420, Loss: 572.6343\n",
      "Epoch: 425, Loss: 572.0106\n",
      "Epoch: 430, Loss: 571.3864\n",
      "Epoch: 435, Loss: 570.7617\n",
      "Epoch: 440, Loss: 570.1366\n",
      "Epoch: 445, Loss: 569.5109\n",
      "Epoch: 450, Loss: 568.8848\n",
      "Epoch: 455, Loss: 568.2583\n",
      "Epoch: 460, Loss: 567.6313\n",
      "Epoch: 465, Loss: 567.0038\n",
      "Epoch: 470, Loss: 566.3757\n",
      "Epoch: 475, Loss: 565.7472\n",
      "Epoch: 480, Loss: 565.1181\n",
      "Epoch: 485, Loss: 564.4885\n",
      "Epoch: 490, Loss: 563.8584\n",
      "Epoch: 495, Loss: 563.2277\n",
      "Epoch: 500, Loss: 562.5966\n",
      "Epoch: 505, Loss: 561.9648\n",
      "Epoch: 510, Loss: 561.3326\n",
      "Epoch: 515, Loss: 560.6998\n",
      "Epoch: 520, Loss: 560.0664\n",
      "Epoch: 525, Loss: 559.4324\n",
      "Epoch: 530, Loss: 558.7979\n",
      "Epoch: 535, Loss: 558.1628\n",
      "Epoch: 540, Loss: 557.5271\n",
      "Epoch: 545, Loss: 556.8908\n",
      "Epoch: 550, Loss: 556.2540\n",
      "Epoch: 555, Loss: 555.6165\n",
      "Epoch: 560, Loss: 554.9784\n",
      "Epoch: 565, Loss: 554.3397\n",
      "Epoch: 570, Loss: 553.7005\n",
      "Epoch: 575, Loss: 553.0605\n",
      "Epoch: 580, Loss: 552.4200\n",
      "Epoch: 585, Loss: 551.7789\n",
      "Epoch: 590, Loss: 551.1370\n",
      "Epoch: 595, Loss: 550.4947\n",
      "Epoch: 600, Loss: 549.8515\n",
      "Epoch: 605, Loss: 549.2079\n",
      "Epoch: 610, Loss: 548.5634\n",
      "Epoch: 615, Loss: 547.9183\n",
      "Epoch: 620, Loss: 547.2726\n",
      "Epoch: 625, Loss: 546.6263\n",
      "Epoch: 630, Loss: 545.9792\n",
      "Epoch: 635, Loss: 545.3315\n",
      "Epoch: 640, Loss: 544.6830\n",
      "Epoch: 645, Loss: 544.0339\n",
      "Epoch: 650, Loss: 543.3842\n",
      "Epoch: 655, Loss: 542.7337\n",
      "Epoch: 660, Loss: 542.0825\n",
      "Epoch: 665, Loss: 541.4307\n",
      "Epoch: 670, Loss: 540.7781\n",
      "Epoch: 675, Loss: 540.1248\n",
      "Epoch: 680, Loss: 539.4707\n",
      "Epoch: 685, Loss: 538.8160\n",
      "Epoch: 690, Loss: 538.1605\n",
      "Epoch: 695, Loss: 537.5042\n",
      "Epoch: 700, Loss: 536.8473\n",
      "Epoch: 705, Loss: 536.1896\n",
      "Epoch: 710, Loss: 535.5311\n",
      "Epoch: 715, Loss: 534.8720\n",
      "Epoch: 720, Loss: 534.2121\n",
      "Epoch: 725, Loss: 533.5513\n",
      "Epoch: 730, Loss: 532.8900\n",
      "Epoch: 735, Loss: 532.2278\n",
      "Epoch: 740, Loss: 531.5648\n",
      "Epoch: 745, Loss: 530.9009\n",
      "Epoch: 750, Loss: 530.2365\n",
      "Epoch: 755, Loss: 529.5712\n",
      "Epoch: 760, Loss: 528.9052\n",
      "Epoch: 765, Loss: 528.2383\n",
      "Epoch: 770, Loss: 527.5707\n",
      "Epoch: 775, Loss: 526.9022\n",
      "Epoch: 780, Loss: 526.2330\n",
      "Epoch: 785, Loss: 525.5630\n",
      "Epoch: 790, Loss: 524.8922\n",
      "Epoch: 795, Loss: 524.2206\n",
      "Epoch: 800, Loss: 523.5482\n",
      "Epoch: 805, Loss: 522.8749\n",
      "Epoch: 810, Loss: 522.2009\n",
      "Epoch: 815, Loss: 521.5260\n",
      "Epoch: 820, Loss: 520.8503\n",
      "Epoch: 825, Loss: 520.1738\n",
      "Epoch: 830, Loss: 519.4965\n",
      "Epoch: 835, Loss: 518.8184\n",
      "Epoch: 840, Loss: 518.1393\n",
      "Epoch: 845, Loss: 517.4595\n",
      "Epoch: 850, Loss: 516.7788\n",
      "Epoch: 855, Loss: 516.0974\n",
      "Epoch: 860, Loss: 515.4150\n",
      "Epoch: 865, Loss: 514.7319\n",
      "Epoch: 870, Loss: 514.0479\n",
      "Epoch: 875, Loss: 513.3630\n",
      "Epoch: 880, Loss: 512.6772\n",
      "Epoch: 885, Loss: 511.9907\n",
      "Epoch: 890, Loss: 511.3033\n",
      "Epoch: 895, Loss: 510.6150\n",
      "Epoch: 900, Loss: 509.9259\n",
      "Epoch: 905, Loss: 509.2359\n",
      "Epoch: 910, Loss: 508.5450\n",
      "Epoch: 915, Loss: 507.8533\n",
      "Epoch: 920, Loss: 507.1607\n",
      "Epoch: 925, Loss: 506.4673\n",
      "Epoch: 930, Loss: 505.7729\n",
      "Epoch: 935, Loss: 505.0777\n",
      "Epoch: 940, Loss: 504.3816\n",
      "Epoch: 945, Loss: 503.6848\n",
      "Epoch: 950, Loss: 502.9868\n",
      "Epoch: 955, Loss: 502.2881\n",
      "Epoch: 960, Loss: 501.5886\n",
      "Epoch: 965, Loss: 500.8882\n",
      "Epoch: 970, Loss: 500.1868\n",
      "Epoch: 975, Loss: 499.4845\n",
      "Epoch: 980, Loss: 498.7814\n",
      "Epoch: 985, Loss: 498.0775\n",
      "Epoch: 990, Loss: 497.3726\n",
      "Epoch: 995, Loss: 496.6667\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f45c33ef927d1145"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
