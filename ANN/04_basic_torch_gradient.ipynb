{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-08T14:59:33.410485Z",
     "start_time": "2024-06-08T14:59:33.408023Z"
    }
   },
   "source": "import torch",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Computational dynamic graph in PyTorch\n",
    "A computational dynamic graph is a data structure that helps to track changes in a tensor using graphs data structures. To active the computational tracking in a PT tensor the tensor must be initialized with the flag `requires_grad=True`. \n",
    "\n",
    "#### *tensor.backward()*\n",
    "The backward method in a PyTorch tensor object set by default the numerical derivative of an actual tensor-operation with respect to a tensor-variable that requires grad. This derivative is stored in the `.grad` property of the tensor-variable that requires grad. \n",
    "\n",
    "This is, with requires_grad=True a tensor-variable is defined as a computational dynamic graph and the operations made with this tensor will be tracked. With the *backward* method this operations will be used to return the derivative of these operations with respect to the tensor-variable.\n",
    "\n",
    "With this concept the derivative of the cost function with respect to the biases and the weights can be calculated thought the deep neural network (in the next sections)."
   ],
   "id": "e7a9441dc12578a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Example 1. Scalar backpropagation\n",
    "With the backward method compute the numerical derivative of the function \n",
    "$$y(x) = 2 x^{4} + x^{3} + 3 x^{2} + 5 x + 1$$\n",
    "with respect to $x$ evaluated in $x = 2$.  \n",
    "\n",
    "Defining a tensor-variable that requires grad as x"
   ],
   "id": "35ef01695d702f14"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T14:59:37.578003Z",
     "start_time": "2024-06-08T14:59:37.575205Z"
    }
   },
   "cell_type": "code",
   "source": "x = torch.tensor(2.0, requires_grad=True)",
   "id": "1edaf774ef67e3cd",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Defining the $y$ tensor-operation using $x$",
   "id": "acb34bcfff556271"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T14:59:42.583414Z",
     "start_time": "2024-06-08T14:59:42.579687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = 2*x**4 + x**3 + 3*x**2 + 5*x + 1\n",
    "print(y)"
   ],
   "id": "fd392085d4962bb0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(63., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The backward method used in y stores the result of the derivative evaluated in 2 in the grad property of x:\n",
    "\n",
    "$$\\frac{dy}{dx}=8(x)^3+3(x)^2+6(x)+5 $$\n",
    "$$\\left. \\frac{dy}{dx} \\right|_{x=2}=8(2)^3+3(2)^2+6(2)+5 = 64+12+12+5 = 93$$"
   ],
   "id": "266dfcc468f96f65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T14:59:51.122304Z",
     "start_time": "2024-06-08T14:59:51.117952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y.backward()\n",
    "print(x.grad)"
   ],
   "id": "8adb6df1f35292a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(93.)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T15:00:04.856020Z",
     "start_time": "2024-06-08T15:00:04.851764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# prove the solution\n",
    "x.grad == 8*x**3 + 3*x**2 + 6*x +5"
   ],
   "id": "35b9899982b5474",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Multistep vector-matrix backpropagation\n",
    "The PyTorch tensor-variable could represent matrices, vectors and scalars. With vectors and matrices pytorch make element-wise operations: operations that are applied independently to each corresponding element in a pair of vectors or matrices. When an operation is performed element-wise, each element in the input(s) undergoes the same operation without considering the structure or dimensions of the input as a whole.\n",
    "\n",
    "Example of element-wise operations\n",
    "\n",
    "Consider the element-wise sum of A + B = C\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\begin{pmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\end{pmatrix}, \\quad \\mathbf{B} = \\begin{pmatrix} b_{11} & b_{12} & b_{13} \\\\ b_{21} & b_{22} & b_{23} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\mathbf{A} + \\mathbf{B} = \\begin{pmatrix} a_{11} + b_{11} & a_{12} + b_{12} & a_{13} + b_{13} \\\\ a_{21} + b_{21} & a_{22} + b_{22} & a_{23} + b_{23} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "A variable-tensor x as dynamic computational graph also track this element wise operations and when these operations are backpropagated the derivatives are also calculated element-wise."
   ],
   "id": "8486f16286ed56fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Example 2. Multistep backpropagation\n",
    "\n",
    "With the matrix $\\mathbf{X}$ make the following operations in pytorch and backpropagate the $\\mathbf{Z}$ tensor\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{pmatrix}\n",
    "1 & 2 &  3 \\\\\n",
    "3 & 2 & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$ \\mathbf{Y} = 3\\mathbf{X} + 2$$\n",
    "\n",
    "$$ \\mathbf{Z} = 2\\mathbf{Y}^2$$"
   ],
   "id": "251c32d964389767"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T15:04:02.646131Z",
     "start_time": "2024-06-08T15:04:02.642423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor([[1.,2,3],[3 ,2 ,1]], requires_grad=True)\n",
    "print(x)"
   ],
   "id": "fa4138407c8435b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [3., 2., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T15:04:03.966812Z",
     "start_time": "2024-06-08T15:04:03.962874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = 3*x + 2\n",
    "print(y)"
   ],
   "id": "5d90588651c150b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.,  8., 11.],\n",
      "        [11.,  8.,  5.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T15:04:04.438815Z",
     "start_time": "2024-06-08T15:04:04.434960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "z = 2*y**2\n",
    "print(z)"
   ],
   "id": "6a34243ef6cb6983",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 50., 128., 242.],\n",
      "        [242., 128.,  50.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With gradient=torch.ones_like(x) the backward method retrieves the gradient taking count the shape of the x tensor, with scalars there is no need for gradient parameter",
   "id": "165ac51f4f3f299c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T15:04:10.238225Z",
     "start_time": "2024-06-08T15:04:10.234709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "z.backward(gradient=torch.ones_like(x))\n",
    "print(x.grad)"
   ],
   "id": "f3d6f81f4c8be27b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 60.,  96., 132.],\n",
      "        [132.,  96.,  60.]])\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Verification \n",
    "\n",
    "Each component of the tensor $z$, as function of $x_i$ can be written as:\n",
    "\n",
    "$$z_i = 2(y_i)^2 = 2(3x_i+2)^2$$\n",
    "\n",
    "To evaluate  $\\frac {\\partial z_i}{\\partial x_i}$ the Chain Rule can be used: $f(g(x)) = f'(g(x))g'(x)$\n",
    "\n",
    "$$f(g(x)) = 2(g(x))^2 $$\n",
    "$$f'(g(x)) = 4g(x) $$\n",
    "$$g(x) = 3x+2$$ \n",
    "$$g'(x) = 3 $$\n",
    "$$\\frac {\\partial z_i}{\\partial x_i} = 4g(x_i) 3 = 12(3x_i+2) $$\n",
    "\n",
    "Evaluating the derivative for each component $x_i$ of the tensor $x$:\n",
    "\n",
    "$$\\left. \\frac{\\partial z_1}{\\partial x_i} \\right|_{x_i=1} = 12(3(1)+2) = 60$$\n",
    "$$\\left. \\frac{\\partial z_2}{\\partial x_i} \\right|_{x_i=2} = 12(3(2)+2) = 96$$\n",
    "$$\\left. \\frac{\\partial z_3}{\\partial x_i} \\right|_{x_i=3} = 12(3(3)+2) = 136$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "11816bbbdd86dcb2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T15:05:01.874215Z",
     "start_time": "2024-06-08T15:05:01.871064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# prove the solution\n",
    "x.grad == 12*(3*x+2)"
   ],
   "id": "e327d6f4ddad71f0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### The use of the mean method in backpropagation\n",
    "In some PyTorch applications it is common to use an average with *mean* before perform the backward pass"
   ],
   "id": "559507c1fd57b1fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T15:07:08.028335Z",
     "start_time": "2024-06-08T15:07:08.025162Z"
    }
   },
   "cell_type": "code",
   "source": "x = torch.tensor([[1.,2,3],[3 ,2 ,1]], requires_grad=True)",
   "id": "a21a3d08c9712576",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T15:07:26.163676Z",
     "start_time": "2024-06-08T15:07:26.160782Z"
    }
   },
   "cell_type": "code",
   "source": "y = 3*x + 2",
   "id": "692fb45e627839f9",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T15:11:26.476970Z",
     "start_time": "2024-06-08T15:11:26.473931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "z = 2*y**2\n",
    "print(z)"
   ],
   "id": "8ec510695362c1de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 50., 128., 242.],\n",
      "        [242., 128.,  50.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T15:08:02.716659Z",
     "start_time": "2024-06-08T15:08:02.712762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out = z.mean()\n",
    "print(out)"
   ],
   "id": "d80ea5854929386a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(140., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T15:08:58.862840Z",
     "start_time": "2024-06-08T15:08:58.859782Z"
    }
   },
   "cell_type": "code",
   "source": "out.backward()",
   "id": "b34076ee21a3e3fc",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T15:09:11.730992Z",
     "start_time": "2024-06-08T15:09:11.728132Z"
    }
   },
   "cell_type": "code",
   "source": "print(x.grad)",
   "id": "b0b7ddaa75461e0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10., 16., 22.],\n",
      "        [22., 16., 10.]])\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now the single component of the Tensor $out = o$, in terms of $z_i(y_i(x_i))$ can be written as:\n",
    "\n",
    "$$  o(x_i) = \\frac {1} {6}\\sum_{i=1}^{6} z_i(y_i(x_i)) $$\n",
    "\n",
    "Taking count that:\n",
    "\n",
    "$$ \\left(\\displaystyle\\sum_{i=1}^nf_i(x)\\right)^\\prime=\\displaystyle\\sum_{i=1}^nf_i^\\prime(x) $$\n",
    "\n",
    "The derivative of $o$ with respect to $x_i$\n",
    "\n",
    "$$ \\frac {\\partial o}{\\partial x_i} = \\left(\\displaystyle \\frac {1} {6} \\sum_{i=1}^6z_i(y_i(x_i))\\right)^\\prime=\\displaystyle \\frac {1} {6} \\sum_{i=1}^6\\frac {\\partial [z_i(y_i(x_i))]} {\\partial x_i} = \\displaystyle \\frac {1} {6} \\sum_{i=1}^6\\frac {\\partial z_i}{\\partial y_i} \\frac {\\partial y_i}{\\partial x_i} $$\n",
    "\n",
    "Pytorch ignores the summation in backward step, then \n",
    "\n",
    "$$\\frac {\\partial o}{\\partial x_i} = \\frac {1} {6} \\frac {\\partial z_i}{\\partial y_i} \\frac {\\partial y_i}{\\partial x_i}  $$\n",
    "\n",
    "With \n",
    "\n",
    "$$\\frac {\\partial z_i}{\\partial y_i} \\frac {\\partial y_i}{\\partial x_i} = \\frac {\\partial z_i}{\\partial x_i} = 12(3x_i+2) $$\n",
    "\n",
    "$$\\frac {\\partial o}{\\partial x_i} = \\frac {1} {6} 12(3x_i+2) = 2(3x_i+2) $$\n",
    "\n"
   ],
   "id": "1d01b73fe38917ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T16:20:03.424457Z",
     "start_time": "2024-06-08T16:20:03.419603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# prove this solution\n",
    "x.grad == 2*(3*x+2)"
   ],
   "id": "3d256f08ff9fff1e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
