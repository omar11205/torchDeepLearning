{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Source: https://github.com/jeffheaton/app_deep_learning/blob/854a0e4a3982daea4e50d34e4d8fc0d9d806b960/t81_558_class_03_4_early_stop.ipynb",
   "id": "e64afea3f913c93a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from IPython.display import Image",
   "id": "97468ac6d2a428a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Early stopping and Network Persistence\n",
    "Early stopping is a technique that helps prevent overfitting and optimize model performance by monitoring validation loss during training. We can avoid un necessary iterations and save computational resources by stopping the training process when the validation loss starts increasing.\n",
    "\n",
    "It can be diffucult to determine how many ephocs to cycle through to train a neural network. Overfitting will occur if you train the neural network for too many ephocs, and the neural network will not perform well on new data, despite attaining a good accuracy on the training set. Overfitting occurs when a neural network is trained to the point that it begins to memorize rather than generalize, as showed in Figrue 1. "
   ],
   "id": "5ac8cf14f5b90742"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Figure 1\n",
    "Image(\"images/training_error_overfitting.png\")"
   ],
   "id": "5d1aedb7a93d849a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "It is important to segment the original dataset into several datasets:\n",
    "\n",
    "- Training Set\n",
    "- Validation Set\n",
    "- Holdout Set\n",
    "\n",
    "You can construct these sets in several different ways. The following programs demonstrate some of these.\n",
    "\n",
    "The first method is a training and validation set. We use the training data to train the neural network until the validation set no longer improves. This attemps to stop at a near-optimal training point. This method will only give accurate \"out of sample\" predictions for the validation set; this is usually 20% of the data. The predictions for the training data will be overly optimistic, as these were the data that we used to train the neural network. Figure 2 represents the data division."
   ],
   "id": "dd6ff1100c18cd5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "Image(\"images/data_division.png\")",
   "id": "e3754cfbc79619a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Because pytorch does not include a built-in early stopping function, we must define one of our own. We will use the following EarlyStopping class. \n",
    "\n",
    "We can provide several parameters to the EarlyStopping object:\n",
    "\n",
    "- min_delta: this value should be kept small; it specifies the minimum change that should be considered an improvement. Setting it even smaller will not likely have a great deal of impact.\n",
    "- patience: how long should the training wait for the validation error to improve?\n",
    "- restore_best_weights: you should usually set this to true, as it restores the weights to the values they were at when the validation set is the highest\n",
    "\n",
    "class definition"
   ],
   "id": "ee7026a02d0cd17b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import copy\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_model = None\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.status = \"\"\n",
    "        \n",
    "    def __call__(self, model, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "        elif self.best_loss - val_loss >= self.min_delta:\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.status = f\"Improvement found, counter reset to {self.counter}\"\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            self.status = f\"NO improvement found, in the last {self.counter} epochs\"\n",
    "            if self.counter >= self.patience:\n",
    "                self.status = f\"Early stopping triggered after {self.counter} epochs\"\n",
    "                if self.restore_best_weights:\n",
    "                    model.load_state_dict(self.best_model) \n",
    "                return True\n",
    "        return False"
   ],
   "id": "258bb9c80f4fb312",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Early stopping with classification \n",
    "We will now see an example of classification training with early stopping. We will train the neural network until the error no longer improve on the validation set."
   ],
   "id": "fb40097eb61ce965"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ],
   "id": "29b39756e4bf7f40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# read the MPG dataset\n",
    "df = pd.read_csv(\"data/auto-mpg.csv\", na_values=[\"NA\", \"?\"])\n",
    "\n",
    "# save the cars name\n",
    "cars = df[\"name\"]"
   ],
   "id": "2a6f44537363dead",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)"
   ],
   "id": "99785877ef271c6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# handle missing values\n",
    "df[\"horsepower\"] = df[\"horsepower\"].fillna(df[\"horsepower\"].median())"
   ],
   "id": "dd6d4dd9cc2bb52e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# select pytorch device\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ],
   "id": "90d8b33842b1f013",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# pandas to numpy \n",
    "x = df[\n",
    "    [\n",
    "        \"cylinders\",\n",
    "        \"displacement\",\n",
    "        \"horsepower\",\n",
    "        \"weight\",\n",
    "        \"acceleration\",\n",
    "        \"year\",\n",
    "        \"origin\"\n",
    "    ]\n",
    "].values.astype(np.float32)\n",
    "\n",
    "y = df[\"mpg\"].values.astype(np.float32)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.25, random_state=42)\n",
    "\n",
    "# numpy to torch tensor\n",
    "x_train = torch.tensor(x_train, device=device, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, device=device, dtype=torch.float32)\n",
    "\n",
    "x_test = torch.tensor(x_test, device=device, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, device=device, dtype=torch.float32)\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "dataset_train = TensorDataset(x_train, y_train)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset_test = TensorDataset(x_test, y_test)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# the model instance must have the name \"model\"\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(x_train.shape[1], 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 25),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(25, 1),\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "es = EarlyStopping()"
   ],
   "id": "4a3f20bd289e8827",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train algorithm\n",
    "epoch = 0\n",
    "done = False\n",
    "while epoch < 100 and not done:\n",
    "    epoch += 1\n",
    "    steps = list(enumerate(dataloader_train))\n",
    "    pbar = tqdm.tqdm(steps)\n",
    "    model.train()\n",
    "    for i, (x_batch, y_batch) in pbar:\n",
    "        # forward pass\n",
    "        y_batch_pred = model(x_batch).flatten()\n",
    "        loss = loss_fn(y_batch_pred, y_batch)\n",
    "        \n",
    "        # restore the gradient for the next calculation\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss = loss.item()\n",
    "        if i == len(steps) - 1:\n",
    "            model.eval()\n",
    "            pred = model(x_test).flatten()\n",
    "            vloss = loss_fn(pred, y_test)\n",
    "            if es(model, vloss):\n",
    "                done = True\n",
    "            pbar.set_description(f\"Epoch: {epoch}, t_loss: {loss:.2f}, v_loss: {vloss:.2f}, EStop:[{es.status}]\")\n",
    "        else: \n",
    "            pbar.set_description(f\"Epoch: {epoch}, t_loss: {loss:.2f}\")"
   ],
   "id": "8372dbbc3082e0be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# measure RMSE error\n",
    "pred = model(x_test)\n",
    "score = torch.sqrt(torch.nn.functional.mse_loss(pred.flatten(), y_test))\n",
    "print(f\"Final score (RMSE): {score}\")"
   ],
   "id": "7a1e8d826cabd76d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Early stopping with classification\n",
    "Example of classification with early stopping. We will train the neural network until the error no longer improves on the validation set."
   ],
   "id": "a271d9662206e45f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T19:45:41.745666Z",
     "start_time": "2024-07-25T19:45:37.816840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ],
   "id": "63c319084d7458d2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T19:45:44.554668Z",
     "start_time": "2024-07-25T19:45:44.527227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"data/iris.csv\", na_values=[\"NA\", \"?\"])\n",
    "le = LabelEncoder()\n",
    "    \n",
    "# extracting the training features\n",
    "x = df[[\"sepal_l\", \"sepal_w\", \"petal_l\", \"petal_w\"]].values\n",
    "    \n",
    "# processing the labels\n",
    "y = le.fit_transform(df[\"species\"])\n",
    "print(y)\n",
    "species = le.classes_"
   ],
   "id": "e65e9728d1b7c0f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# split into validation and training sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ],
   "id": "5904310df40631e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The statement scaler = StandardScaler() is used to standardize the features of the dataset before training the neural network. The StandardScaler is a preprocessing technique from the scikit-learn library, which standardizes features by removing the mean and scaling to unit variance.\n",
    "\n",
    "Why Standardize the Features?\n",
    "- Consistent Scale: Neural networks are sensitive to the scale of the input features. Features with vastly different scales can lead to an unstable training process. Standardizing the data ensures that each feature contributes equally to the training process by having a consistent scale.\n",
    "\n",
    "- Faster Convergence: Standardized data can help the optimizer converge faster to a minimum, as it can navigate the cost function's surface more effectively when the input features are on a similar scale.\n",
    "\n",
    "- Improved Performance: Standardization can often lead to improved model performance, as it prevents certain features from dominating the learning process solely due to their scale.\n",
    "\n",
    "scaler.fit_transform(x_train): Fits the scaler to the training data and transforms it, standardizing the features by subtracting the mean and scaling to unit variance.\n",
    "\n",
    "scaler.transform(x_test): Transforms the test data using the parameters (mean and variance) computed from the training data. This ensures that the test set is scaled in the same way as the training set, maintaining consistency.\n",
    "This process helps the neural network learn effectively and improves the overall model's robustness and accuracy."
   ],
   "id": "8d9a10fec7239fae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "# numpy to torch tensor\n",
    "x_train = torch.tensor(x_train, device=device, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, device=device, dtype=torch.long)\n",
    "\n",
    "x_test = torch.tensor(x_test, device=device, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, device=device, dtype=torch.long)\n",
    "\n",
    "# create torch datasets\n",
    "batch_size = 16\n",
    "\n",
    "dataset_train = TensorDataset(x_train, y_train)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset_test = TensorDataset(x_test, y_test)\n",
    "dataloader_test = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)"
   ],
   "id": "cdba5bf919184428",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# create the model using Sequential\n",
    "print(x_train.shape[1])\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(x_train.shape[1], 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 25),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(25, len(species)),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")"
   ],
   "id": "eaa5adfbff2697f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# loss function definition\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer definition\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# early stop instance\n",
    "early_stop = EarlyStopping()"
   ],
   "id": "4ca02b8c5a0c64df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train algorithm\n",
    "epoch = 0\n",
    "done = False\n",
    "while epoch < 100 and not done:\n",
    "    epoch += 1\n",
    "    steps = list(enumerate(dataloader_train)) # list of tuples (batch_id, [x_batch_id, y_batch_id]) \n",
    "    pbar = tqdm.tqdm(steps)\n",
    "    model.train()\n",
    "    for i, (x_batch, y_batch) in pbar:\n",
    "        y_batch_pred = model(x_batch.to(device))\n",
    "        loss = loss_fn(y_batch_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss = loss.item()\n",
    "        # if the control get the last batch of the current epoch \n",
    "        if i == len(steps) - 1:\n",
    "            model.eval()\n",
    "            pred = model(x_test)\n",
    "            vloss = loss_fn(pred, y_test)\n",
    "            if early_stop(model, vloss):\n",
    "                done = True\n",
    "            pbar.set_description(f\"Epoch: {epoch}, t_loss {loss:.2f}, v_loss {vloss:.2f}, {early_stop.status}\")\n",
    "        else:\n",
    "            pbar.set_description(f\"Epoch: {epoch}, t_loss {loss:.2f}\")"
   ],
   "id": "777501049f2256fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pred = model(x_test)\n",
    "vloss = loss_fn(pred, y_test)\n",
    "print(f\"Loss = {vloss}\")"
   ],
   "id": "79a7913f57af75d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5824042ba5c27793",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
