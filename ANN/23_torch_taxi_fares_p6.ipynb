{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-02T02:02:13.351920Z",
     "start_time": "2024-09-02T02:02:08.707142Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T02:02:56.004728Z",
     "start_time": "2024-09-02T02:02:55.963515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# setting device and reproducibility\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42)\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "d107944b81323fde",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Final training with all the hyperparameters\n",
    "- Activation function: *SmothL1Loss*\n",
    "- Optimizer: *Adam*\n",
    "- Learning Rate: $0.001$\n",
    "- Network Architecture: $[400,300,200,100]$\n",
    "- Dropout Probability: $0.2$\n",
    "- Train Epochs: $[150]$\n",
    "\n",
    "### Preprocessing"
   ],
   "id": "b32025fc0bb7ab1f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T02:03:09.450112Z",
     "start_time": "2024-09-02T02:03:09.214900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import data \n",
    "data_frame = pd.read_csv(\"data/NYCTaxiFares.csv\", na_values=[\"NA\", \"?\"])"
   ],
   "id": "af1b85ef9231104f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T02:03:10.948056Z",
     "start_time": "2024-09-02T02:03:10.923543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function to calculate the distance of the travel\n",
    "def haversine_distance(dat_f, lat1, lon1, lat2, lon2):\n",
    "    \n",
    "    # average radius of the Earth in (km)\n",
    "    r = 6371\n",
    "    \n",
    "    phi1 = np.radians(dat_f[lat1])\n",
    "    phi2 = np.radians(dat_f[lat2])\n",
    "    delta_phi = np.radians(dat_f[lat2] - dat_f[lat1])\n",
    "    delta_lambda = np.radians(dat_f[lon2] - dat_f[lon1])\n",
    "    \n",
    "    a = np.sin(delta_phi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    d = (r * c)\n",
    "    \n",
    "    return d\n",
    "\n",
    "def preprocessing(df_n, cat_cols):\n",
    "    \"\"\"\n",
    "    Preprocesses the data and adds pandas categorical fields to a dataframe.\n",
    "    :param df_n: pandas dataframe \n",
    "    :param cat_cols: list of categorical fields\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    # append a 'dist_km' new feature in the dataframe\n",
    "    df_n['dist_km'] = haversine_distance(df_n, 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude')\n",
    "    \n",
    "    # remove outliers\n",
    "    dfd = df_n[(df_n['fare_amount'] != 49.57) & (df_n['fare_amount'] != 45.00)].copy()\n",
    "    \n",
    "    # convert to pd datetime\n",
    "    dfd['pickup_datetime'] = pd.to_datetime(dfd['pickup_datetime'])\n",
    "    \n",
    "    # Correcting pickup_datetime due to daylight savings time (April)\n",
    "    dfd['EDTdate'] = dfd['pickup_datetime'] - pd.Timedelta(hours=4)\n",
    "    \n",
    "    # create new time fields\n",
    "    dfd['Hour'] = dfd['EDTdate'].dt.hour\n",
    "    dfd['AMorPM'] = np.where(dfd['Hour']<12, 'am', 'pm')\n",
    "    dfd['Weekday'] = dfd['EDTdate'].dt.strftime(\"%a\")\n",
    "    \n",
    "    # transform to pandas categorical variables\n",
    "    for cat in cat_cols:\n",
    "        dfd[cat] = dfd[cat].astype('category')\n",
    "    \n",
    "    dfd = dfd.drop(columns=['pickup_datetime'])\n",
    "    \n",
    "    return dfd\n",
    "\n",
    "def model_tensors(df, cat_cols, cont_cols, y_col):\n",
    "    \"\"\"\n",
    "    Get categorical, continuous and label tensors for the model\n",
    "    :param df: pd dataframe\n",
    "    :param cat_cols: list of categorical fields\n",
    "    :param cont_cols: list of continuous fields\n",
    "    :param y_col: list with the labels\n",
    "    :return: cats, conts, y tensors\n",
    "    \"\"\"\n",
    "    \n",
    "    # group the data in categorical continuous and target label    \n",
    "    cats = np.stack([df[col].cat.codes.values for col in cat_cols], axis=1)\n",
    "    conts = np.stack([df[col].values for col in cont_cols], axis=1)\n",
    "    y = df[y_col].values.reshape(-1, 1)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    cats_t = torch.tensor(cats, dtype=torch.int64)\n",
    "    conts_t = torch.tensor(conts, dtype=torch.float32)\n",
    "    y_t = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    return cats_t, conts_t, y_t\n",
    "\n",
    "def create_embedding_sizes(df, cat_cols):\n",
    "    \"\"\"\n",
    "    Create embedding sizes for PyTorch embedding layers\n",
    "    :param df: pandas dataframe\n",
    "    :param cat_cols: list of categorical fields\n",
    "    :return: emb_sizes list\n",
    "    \"\"\"\n",
    "    # categorical sizes list\n",
    "    cat_sizes = [len(df[col].cat.categories) for col in cat_cols]\n",
    "\n",
    "    # embedding sizes list (divide the number of unique entries in each column by two, if the result is greater than 50 select 50)\n",
    "    emb_sizes = [(size, min(50,(size+1)//2)) for size in cat_sizes]\n",
    "    \n",
    "    return emb_sizes\n"
   ],
   "id": "edaf7499a22f5678",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T02:03:18.157193Z",
     "start_time": "2024-09-02T02:03:12.859176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = preprocessing(data_frame, ['Hour', 'AMorPM', 'Weekday'])\n",
    "\n",
    "cats, conts, y = model_tensors(df, ['Hour', 'AMorPM', 'Weekday'], ['dist_km', 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'], ['fare_amount'])\n",
    "\n",
    "# number of continuous fields of the conts tensor\n",
    "n_cont = conts.shape[1]\n",
    "\n",
    "emb_sizes = create_embedding_sizes(df, ['Hour', 'AMorPM', 'Weekday'])"
   ],
   "id": "959204a9313e248",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model definition",
   "id": "d074c010ea3ecbc6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T02:03:20.190141Z",
     "start_time": "2024-09-02T02:03:20.170628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the model\n",
    "class TabularModel(nn.Module):\n",
    "    def __init__(self, emb_sizes, n_cont, out_size, layers, p=0.5):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in emb_sizes])\n",
    "        self.emb_drop = nn.Dropout(p)\n",
    "        self.batch_norm_cont = nn.BatchNorm1d(n_cont)\n",
    "\n",
    "        layer_list = []\n",
    "        n_emb = sum([nf for ni, nf in emb_sizes])\n",
    "        n_in = n_emb + n_cont\n",
    "        for i in layers:\n",
    "            layer_list.append(nn.Linear(n_in, i))\n",
    "            layer_list.append(nn.ReLU(inplace=True))\n",
    "            layer_list.append(nn.BatchNorm1d(i))\n",
    "            layer_list.append(nn.Dropout(p))\n",
    "            n_in = i\n",
    "\n",
    "        layer_list.append(nn.Linear(layers[-1], out_size))\n",
    "        self.layers = nn.Sequential(*layer_list)\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        embeddings = [e(x_cat[:, i]) for i, e in enumerate(self.embeds)]\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        x_cont = self.batch_norm_cont(x_cont)\n",
    "        x = torch.cat([x, x_cont], 1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ],
   "id": "ad20c477b64ac00b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Split Function\n",
    "Function to split the dataset in 6. `Train data`: cat_train, con_train, y_train; `Test Data`: cat_test, con_test, y_test"
   ],
   "id": "b1d2e2c5ca7f73b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T02:03:22.167246Z",
     "start_time": "2024-09-02T02:03:22.095554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_train_test(categoricals, continuous, y_train, test_size=0.2):\n",
    "    # Ensure the input arrays have the same number of rows\n",
    "    assert categoricals.shape[0] == continuous.shape[0] == y_train.shape[0], \"Input arrays must have the same number of rows\"\n",
    "\n",
    "    # Combine the data into a single array for splitting\n",
    "    combined = np.hstack((categoricals, continuous, y_train))\n",
    "\n",
    "    # Split and shuffle combined data into train and test sets\n",
    "    train_data, test_data = train_test_split(combined, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Determine the number of categorical and continuous columns\n",
    "    n_cat_cols = categoricals.shape[1]\n",
    "    n_cont_cols = continuous.shape[1]\n",
    "\n",
    "    # Separate the train and test data back into categorical, continuous, and target tensors\n",
    "\n",
    "    # selects all rows and the first n_cat_cols columns (categorical features).\n",
    "    cat_train = torch.tensor(train_data[:, :n_cat_cols], dtype=torch.int64).to(device)\n",
    "\n",
    "    # selects all rows and the columns from n_cat_cols to n_cat_cols + n_cont_cols (continuous features).\n",
    "    con_train = torch.tensor(train_data[:, n_cat_cols:n_cat_cols + n_cont_cols], dtype=torch.float32).to(device)\n",
    "\n",
    "    # selects all rows and the last column (target labels).\n",
    "    y_train = torch.tensor(train_data[:, -1], dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    cat_test = torch.tensor(test_data[:, :n_cat_cols], dtype=torch.int64).to(device)\n",
    "    con_test = torch.tensor(test_data[:, n_cat_cols:n_cat_cols + n_cont_cols], dtype=torch.float32).to(device)\n",
    "    y_test = torch.tensor(test_data[:, -1], dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    return cat_train, con_train, y_train, cat_test, con_test, y_test\n",
    "\n",
    "cat_train, con_train, y_train, cat_test, con_test, y_test = get_train_test(cats, conts, y)"
   ],
   "id": "ff4cb4dc2aebe1d2",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T02:03:27.325823Z",
     "start_time": "2024-09-02T02:03:24.899903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model instance (output_size = 1 for regression task)\n",
    "model = TabularModel(emb_sizes, conts.shape[1], 1, [400, 300, 200, 100], p=0.2).to(device)\n",
    "\n",
    "# criteria\n",
    "criterion_1 = nn.SmoothL1Loss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# TensorDatasets\n",
    "train_dataset = TensorDataset(cat_train, con_train, y_train)\n",
    "test_dataset = TensorDataset(cat_test, con_test, y_test)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# results\n",
    "results = []"
   ],
   "id": "16718fb8730520f3",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T02:23:46.442294Z",
     "start_time": "2024-09-02T02:07:53.285392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "epochs = 150\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "\n",
    "    for cat_batch, con_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(cat_batch, con_batch).flatten()\n",
    "        loss = torch.sqrt(criterion_1(y_pred, y_batch.flatten()))  # Use y_batch.flatten() here\n",
    "        epoch_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_losses.append(np.mean(epoch_losses))\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_val_loss = []\n",
    "        for cat_batch, con_batch, y_batch in test_loader:\n",
    "            y_val = model(cat_batch, con_batch).flatten()\n",
    "            val_loss = torch.sqrt(criterion_1(y_val, y_batch.flatten()))  # Use y_batch.flatten() here\n",
    "            epoch_val_loss.append(val_loss.item())\n",
    "\n",
    "        val_losses.append(np.mean(epoch_val_loss))\n",
    "\n",
    "    if epoch % 25 == 0 or epoch == epochs - 1:\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Validation Loss: {val_losses[-1]:.4f}')\n",
    "    \n",
    "    results.append({\n",
    "        \"Epoch\": epoch + 1,\n",
    "        \"Train Loss\": train_losses[-1],\n",
    "        \"Validation Loss\": val_losses[-1],\n",
    "    })\n",
    "    \n",
    "\n",
    "print(f'Training completed in {time.time() - start_time:.2f} seconds')"
   ],
   "id": "12099e63027f28aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150, Train Loss: 1.5953, Validation Loss: 1.0606\n",
      "Epoch 26/150, Train Loss: 1.0273, Validation Loss: 0.9307\n",
      "Epoch 51/150, Train Loss: 1.0055, Validation Loss: 0.9176\n",
      "Epoch 76/150, Train Loss: 0.9960, Validation Loss: 0.9182\n",
      "Epoch 101/150, Train Loss: 0.9895, Validation Loss: 0.9039\n",
      "Epoch 126/150, Train Loss: 0.9856, Validation Loss: 0.9049\n",
      "Epoch 150/150, Train Loss: 0.9810, Validation Loss: 0.9228\n",
      "Training completed in 953.14 seconds\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T02:32:35.596537Z",
     "start_time": "2024-09-02T02:32:35.455009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TO EVALUATE THE ENTIRE TEST SET\n",
    "with torch.no_grad():\n",
    "    y_val = model(cat_test, con_test).flatten()\n",
    "    loss = torch.sqrt(criterion_1(y_val, y_test.flatten()))\n",
    "print(f'RMSE: {loss:.8f}')"
   ],
   "id": "51a801af04a649fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.92663044\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f'{\"PREDICTED\":>12} {\"ACTUAL\":>8} {\"DIFF\":>8}')\n",
    "for i in range(50):\n",
    "    diff = np.abs(y_val[i].item()-y_test[i].item())\n",
    "    print(f'{i+1:2}. {y_val[i].item():8.4f} {y_test[i].item():8.4f} {diff:8.4f}')"
   ],
   "id": "d8496ce014887e99"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###     Save the model\n",
    "The best practice is to save the state of the model (weights & biases) and not the full definition. Also, we want to ensure that only a trained model is saved, to prevent overwritting a previously saved model with an untraited one."
   ],
   "id": "b3589a9dd009624e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# make sure to save the model only after the final training is complete\n",
    "if len(val_losses) == epochs:\n",
    "    torch.save(model.state_dict(), f\"export/trained_model_{epochs}.pth\")\n",
    "else:\n",
    "    print('Model has not been trained. Load another model?')"
   ],
   "id": "66d83cb6c91f8225"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
