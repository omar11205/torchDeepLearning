{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-24T18:59:34.160382Z",
     "start_time": "2024-08-24T18:59:31.901477Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from class_models import early_stop\n",
    "import time\n",
    "from torch.optim.lr_scheduler import StepLR"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T18:59:36.675855Z",
     "start_time": "2024-08-24T18:59:36.622719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# setting device and reproducibility\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42)\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "8b6358891ee8e788",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Learning schedule to find the best learning rate using *StepLR* scheduler",
   "id": "e47ae840dd103484"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T19:01:22.809300Z",
     "start_time": "2024-08-24T19:01:22.678609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import data \n",
    "data_frame = pd.read_csv(\"data/NYCTaxiFares.csv\", na_values=[\"NA\", \"?\"])"
   ],
   "id": "5aa4b73fe3e92c46",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T19:01:25.279059Z",
     "start_time": "2024-08-24T19:01:25.189638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function to calculate the distance of the travel\n",
    "def haversine_distance(dat_f, lat1, lon1, lat2, lon2):\n",
    "    \n",
    "    # average radius of the Earth in (km)\n",
    "    r = 6371\n",
    "    \n",
    "    phi1 = np.radians(dat_f[lat1])\n",
    "    phi2 = np.radians(dat_f[lat2])\n",
    "    delta_phi = np.radians(dat_f[lat2] - dat_f[lat1])\n",
    "    delta_lambda = np.radians(dat_f[lon2] - dat_f[lon1])\n",
    "    \n",
    "    a = np.sin(delta_phi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    d = (r * c)\n",
    "    \n",
    "    return d\n",
    "\n",
    "def preprocessing(df_n, cat_cols):\n",
    "    \"\"\"\n",
    "    Preprocesses the data and adds pandas categorical fields to a dataframe.\n",
    "    :param df_n: pandas dataframe \n",
    "    :param cat_cols: list of categorical fields\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    # append a 'dist_km' new feature in the dataframe\n",
    "    df_n['dist_km'] = haversine_distance(df_n, 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude')\n",
    "    \n",
    "    # remove outliers\n",
    "    dfd = df_n[(df_n['fare_amount'] != 49.57) & (df_n['fare_amount'] != 45.00)].copy()\n",
    "    \n",
    "    # convert to pd datetime\n",
    "    dfd['pickup_datetime'] = pd.to_datetime(dfd['pickup_datetime'])\n",
    "    \n",
    "    # Correcting pickup_datetime due to daylight savings time (April)\n",
    "    dfd['EDTdate'] = dfd['pickup_datetime'] - pd.Timedelta(hours=4)\n",
    "    \n",
    "    # create new time fields\n",
    "    dfd['Hour'] = dfd['EDTdate'].dt.hour\n",
    "    dfd['AMorPM'] = np.where(dfd['Hour']<12, 'am', 'pm')\n",
    "    dfd['Weekday'] = dfd['EDTdate'].dt.strftime(\"%a\")\n",
    "    \n",
    "    # transform to pandas categorical variables\n",
    "    for cat in cat_cols:\n",
    "        dfd[cat] = dfd[cat].astype('category')\n",
    "    \n",
    "    dfd = dfd.drop(columns=['pickup_datetime'])\n",
    "    \n",
    "    return dfd\n",
    "\n",
    "def model_tensors(df, cat_cols, cont_cols, y_col):\n",
    "    \"\"\"\n",
    "    Get categorical, continuous and label tensors for the model\n",
    "    :param df: pd dataframe\n",
    "    :param cat_cols: list of categorical fields\n",
    "    :param cont_cols: list of continuous fields\n",
    "    :param y_col: list with the labels\n",
    "    :return: cats, conts, y tensors\n",
    "    \"\"\"\n",
    "    \n",
    "    # group the data in categorical continuous and target label    \n",
    "    cats = np.stack([df[col].cat.codes.values for col in cat_cols], axis=1)\n",
    "    conts = np.stack([df[col].values for col in cont_cols], axis=1)\n",
    "    y = df[y_col].values.reshape(-1, 1)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    cats_t = torch.tensor(cats, dtype=torch.int64)\n",
    "    conts_t = torch.tensor(conts, dtype=torch.float32)\n",
    "    y_t = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    return cats_t, conts_t, y_t\n",
    "\n",
    "def create_embedding_sizes(df, cat_cols):\n",
    "    \"\"\"\n",
    "    Create embedding sizes for PyTorch embedding layers\n",
    "    :param df: pandas dataframe\n",
    "    :param cat_cols: list of categorical fields\n",
    "    :return: emb_sizes list\n",
    "    \"\"\"\n",
    "    # categorical sizes list\n",
    "    cat_sizes = [len(df[col].cat.categories) for col in cat_cols]\n",
    "\n",
    "    # embedding sizes list (divide the number of unique entries in each column by two, if the result is greater than 50 select 50)\n",
    "    emb_sizes = [(size, min(50,(size+1)//2)) for size in cat_sizes]\n",
    "    \n",
    "    return emb_sizes\n",
    "\n"
   ],
   "id": "9b52fe568f07205e",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T19:01:29.060948Z",
     "start_time": "2024-08-24T19:01:27.782845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = preprocessing(data_frame, ['Hour', 'AMorPM', 'Weekday'])\n",
    "\n",
    "cats, conts, y = model_tensors(df, ['Hour', 'AMorPM', 'Weekday'], ['dist_km', 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'], ['fare_amount'])\n",
    "\n",
    "# number of continuous fields of the conts tensor\n",
    "n_cont = conts.shape[1]\n",
    "\n",
    "emb_sizes = create_embedding_sizes(df, ['Hour', 'AMorPM', 'Weekday'])"
   ],
   "id": "6415281a4de6470",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model definition",
   "id": "fe20a455c5e22d44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T19:01:31.467987Z",
     "start_time": "2024-08-24T19:01:31.462987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the model\n",
    "class TabularModel(nn.Module):\n",
    "    def __init__(self, emb_sizes, n_cont, out_size, layers, p=0.5):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in emb_sizes])\n",
    "        self.emb_drop = nn.Dropout(p)\n",
    "        self.batch_norm_cont = nn.BatchNorm1d(n_cont)\n",
    "\n",
    "        layer_list = []\n",
    "        n_emb = sum([nf for ni, nf in emb_sizes])\n",
    "        n_in = n_emb + n_cont\n",
    "        for i in layers:\n",
    "            layer_list.append(nn.Linear(n_in, i))\n",
    "            layer_list.append(nn.ReLU(inplace=True))\n",
    "            layer_list.append(nn.BatchNorm1d(i))\n",
    "            layer_list.append(nn.Dropout(p))\n",
    "            n_in = i\n",
    "\n",
    "        layer_list.append(nn.Linear(layers[-1], out_size))\n",
    "        self.layers = nn.Sequential(*layer_list)\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        embeddings = [e(x_cat[:, i]) for i, e in enumerate(self.embeds)]\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        x_cont = self.batch_norm_cont(x_cont)\n",
    "        x = torch.cat([x, x_cont], 1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ],
   "id": "e7bc0aa7a2948d56",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Split Function\n",
    "Function to split the dataset in 6"
   ],
   "id": "716c3d6415d5e47b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T21:10:37.177895Z",
     "start_time": "2024-08-24T21:10:37.145744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_train_test(categoricals, continuous, y_train, test_size=0.2):\n",
    "    # Ensure the input arrays have the same number of rows\n",
    "    assert categoricals.shape[0] == continuous.shape[0] == y_train.shape[0], \"Input arrays must have the same number of rows\"\n",
    "\n",
    "    # Combine the data into a single array for splitting\n",
    "    combined = np.hstack((categoricals, continuous, y_train))\n",
    "\n",
    "    # Split the combined data into train and test sets\n",
    "    train_data, test_data = train_test_split(combined, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Determine the number of categorical and continuous columns\n",
    "    n_cat_cols = categoricals.shape[1]\n",
    "    n_cont_cols = continuous.shape[1]\n",
    "\n",
    "    # Separate the train and test data back into categorical, continuous, and target tensors\n",
    "\n",
    "    # selects all rows and the first n_cat_cols columns (categorical features).\n",
    "    cat_train = torch.tensor(train_data[:, :n_cat_cols], dtype=torch.int64).to(device)\n",
    "\n",
    "    # selects all rows and the columns from n_cat_cols to n_cat_cols + n_cont_cols (continuous features).\n",
    "    con_train = torch.tensor(train_data[:, n_cat_cols:n_cat_cols + n_cont_cols], dtype=torch.float32).to(device)\n",
    "\n",
    "    # selects all rows and the last column (target labels).\n",
    "    y_train = torch.tensor(train_data[:, -1], dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    cat_test = torch.tensor(test_data[:, :n_cat_cols], dtype=torch.int64).to(device)\n",
    "    con_test = torch.tensor(test_data[:, n_cat_cols:n_cat_cols + n_cont_cols], dtype=torch.float32).to(device)\n",
    "    y_test = torch.tensor(test_data[:, -1], dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    return cat_train, con_train, y_train, cat_test, con_test, y_test\n",
    "\n",
    "cat_train, con_train, y_train, cat_test, con_test, y_test = get_train_test(cats, conts, y)"
   ],
   "id": "78fbc603770e3584",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Model and training parameters\n",
    "With the aim of contrast the performance of the learning with a constant learning rate vs the performance of the training with a learning rate schedule  "
   ],
   "id": "3e7f5652ef3b64d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T21:12:18.891695Z",
     "start_time": "2024-08-24T21:12:18.882553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model instance (output_size = 1 for regression task)\n",
    "model = TabularModel(emb_sizes, conts.shape[1], 1, [400, 300, 200, 100], p=0.4).to(device)\n",
    "print(model.layers)"
   ],
   "id": "55b7b3f12cdb975b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=22, out_features=400, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (3): Dropout(p=0.4, inplace=False)\n",
      "  (4): Linear(in_features=400, out_features=300, bias=True)\n",
      "  (5): ReLU(inplace=True)\n",
      "  (6): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (7): Dropout(p=0.4, inplace=False)\n",
      "  (8): Linear(in_features=300, out_features=200, bias=True)\n",
      "  (9): ReLU(inplace=True)\n",
      "  (10): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (11): Dropout(p=0.4, inplace=False)\n",
      "  (12): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (15): Dropout(p=0.4, inplace=False)\n",
      "  (16): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### General Configuration for all trainings",
   "id": "e298278f101290fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T21:12:27.753683Z",
     "start_time": "2024-08-24T21:12:27.749921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# criteria\n",
    "criterion = nn.SmoothL1Loss()\n",
    "\n",
    "# TensorDatasets\n",
    "train_dataset = TensorDataset(cat_train, con_train, y_train)\n",
    "test_dataset = TensorDataset(cat_test, con_test, y_test)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# function to reset weights\n",
    "def reset_weights(m):\n",
    "    if hasattr(m, 'reset_parameters'):\n",
    "        m.reset_parameters()\n",
    "\n"
   ],
   "id": "a6ac861eeaadc06e",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Defining a training function for learning schedules ",
   "id": "fcdb5be4d2efddbc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T21:12:29.901613Z",
     "start_time": "2024-08-24T21:12:29.894054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training function\n",
    "def train(epochs, train_schedule, start_lr, early_s_patience, print_rate):\n",
    "    \"\"\"\n",
    "    :param epochs: int, number of training epochs\n",
    "    :param train_schedule: list of lists, [[mode, scheduler_step_size, reduction_factor_gamma]]\n",
    "    :param start_lr: float, starting learning rate\n",
    "    :param early_s_patience: int, early stopping patience\n",
    "    :param print_rate: int, printing rate\n",
    "    :return: Pandas.DataFrame, the results of the training\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    for mode, step_size, gamma in train_schedule:\n",
    "        print(f\"Training mode: {mode}\")\n",
    "        \n",
    "        # restart optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=start_lr)\n",
    "        \n",
    "        if mode == \"LR Scheduler\":\n",
    "            # restart learning rate scheduler\n",
    "            scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "            \n",
    "        # reset model parameters\n",
    "        model.apply(reset_weights)\n",
    "        \n",
    "        # reset early stop\n",
    "        early_stopping = early_stop.EarlyStopping(patience=early_s_patience)\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "            \n",
    "            model.train()\n",
    "            epoch_losses = []\n",
    "            for cat_batch, con_batch, y_batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(cat_batch, con_batch).flatten()\n",
    "                loss = torch.sqrt(criterion(y_pred, y_batch.flatten()))  # Use y_batch.flatten() here\n",
    "                epoch_losses.append(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "            train_losses.append(np.mean(epoch_losses))\n",
    "        \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                epoch_val_loss = []\n",
    "                for cat_batch, con_batch, y_batch in test_loader:\n",
    "                    y_val = model(cat_batch, con_batch).flatten()\n",
    "                    val_loss = torch.sqrt(criterion(y_val, y_batch.flatten()))  # Use y_batch.flatten() here\n",
    "                    epoch_val_loss.append(val_loss.item())\n",
    "        \n",
    "                val_losses.append(np.mean(epoch_val_loss))\n",
    "        \n",
    "            if (epoch % print_rate == 0 or epoch == epochs - 1) and mode == \"LR Scheduler\":\n",
    "                print(f'Epoch {epoch + 1}/{epochs}, T. Loss: {train_losses[-1]:.4f}, V. Loss: {val_losses[-1]:.4f}, Early S: {early_stopping.status}, Gamma: {gamma}, S. Size: {step_size}, L.R. {optimizer.param_groups[0]['lr']:.6f}')\n",
    "            elif epoch % print_rate == 0 or epoch == epochs - 1: \n",
    "                print(f'Epoch {epoch + 1}/{epochs}, T. Loss: {train_losses[-1]:.4f}, V. Loss: {val_losses[-1]:.4f}, Early S: {early_stopping.status}, L.R. {optimizer.param_groups[0]['lr']:.6f}')\n",
    "            \n",
    "            results.append({\n",
    "                \"Mode\": mode,\n",
    "                \"Gamma\": gamma,\n",
    "                \"Step\": step_size,\n",
    "                \"Epoch\": epoch + 1,\n",
    "                \"Learning Rate\": optimizer.param_groups[0]['lr'],\n",
    "                \"Train Loss\": train_losses[-1],\n",
    "                \"Validation Loss\": val_losses[-1],\n",
    "            })\n",
    "            \n",
    "            if mode == \"LR Scheduler\":\n",
    "                scheduler.step()\n",
    "            \n",
    "            # Check early stopping\n",
    "            if early_stopping(model, val_losses[-1]):  # Pass the latest validation loss to early stopping\n",
    "                print(early_stopping.status)\n",
    "                break\n",
    "        \n",
    "        print(f'Training completed in {time.time() - start_time:.2f} seconds')\n",
    "\n",
    "    return pd.DataFrame(results)"
   ],
   "id": "19d52c6a0d1d495b",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train Schedule 1: ",
   "id": "f3aeb3796dbaf8a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T21:43:37.640043Z",
     "start_time": "2024-08-24T21:12:32.487967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_epochs = 100\n",
    "train_lr_schedule = [\n",
    "    [\"LR Scheduler\", 25, 0.9],\n",
    "    [\"LR Scheduler\", 20, 0.9],\n",
    "    [\"LR Scheduler\", 15, 0.9],\n",
    "    [\"LR Scheduler\", 10, 0.9],\n",
    "    [\"LR Scheduler\", 5, 0.9]\n",
    "]\n",
    "starting_lr = 0.01\n",
    "early_stop_patience = 40\n",
    "printing_rate = 20\n",
    "\n",
    "results_lr_09 = train(training_epochs, train_lr_schedule, starting_lr, early_stop_patience, printing_rate)"
   ],
   "id": "1092800ad9109b87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mode: LR Scheduler\n",
      "Epoch 1/100, T. Loss: 1.2868, V. Loss: 1.0874, Early S: , Gamma: 0.9, S. Size: 25, L.R. 0.010000\n",
      "Epoch 21/100, T. Loss: 1.0651, V. Loss: 0.9531, Early S: Improvement!!!, actual counter 3, Gamma: 0.9, S. Size: 25, L.R. 0.010000\n",
      "Epoch 41/100, T. Loss: 1.0591, V. Loss: 0.9533, Early S: Improvement!!!, actual counter 6, Gamma: 0.9, S. Size: 25, L.R. 0.009000\n",
      "Epoch 61/100, T. Loss: 1.0522, V. Loss: 0.9344, Early S: NO improvement in the last 8 epochs, Gamma: 0.9, S. Size: 25, L.R. 0.008100\n",
      "Epoch 81/100, T. Loss: 1.0380, V. Loss: 0.9265, Early S: NO improvement in the last 11 epochs, Gamma: 0.9, S. Size: 25, L.R. 0.007290\n",
      "Epoch 100/100, T. Loss: 1.0418, V. Loss: 0.9821, Early S: NO improvement in the last 16 epochs, Gamma: 0.9, S. Size: 25, L.R. 0.007290\n",
      "Training completed in 380.13 seconds\n",
      "Training mode: LR Scheduler\n",
      "Epoch 1/100, T. Loss: 1.2817, V. Loss: 1.0213, Early S: , Gamma: 0.9, S. Size: 20, L.R. 0.010000\n",
      "Epoch 21/100, T. Loss: 1.0725, V. Loss: 0.9597, Early S: Improvement!!!, actual counter 0, Gamma: 0.9, S. Size: 20, L.R. 0.009000\n",
      "Epoch 41/100, T. Loss: 1.0575, V. Loss: 0.9458, Early S: NO improvement in the last 6 epochs, Gamma: 0.9, S. Size: 20, L.R. 0.008100\n",
      "Epoch 61/100, T. Loss: 1.0460, V. Loss: 0.9629, Early S: Improvement!!!, actual counter 4, Gamma: 0.9, S. Size: 20, L.R. 0.007290\n",
      "Epoch 81/100, T. Loss: 1.0409, V. Loss: 0.9433, Early S: NO improvement in the last 15 epochs, Gamma: 0.9, S. Size: 20, L.R. 0.006561\n",
      "Epoch 100/100, T. Loss: 1.0345, V. Loss: 0.9705, Early S: NO improvement in the last 2 epochs, Gamma: 0.9, S. Size: 20, L.R. 0.006561\n",
      "Training completed in 760.77 seconds\n",
      "Training mode: LR Scheduler\n",
      "Epoch 1/100, T. Loss: 1.2886, V. Loss: 1.0434, Early S: , Gamma: 0.9, S. Size: 15, L.R. 0.010000\n",
      "Epoch 21/100, T. Loss: 1.0698, V. Loss: 0.9502, Early S: NO improvement in the last 3 epochs, Gamma: 0.9, S. Size: 15, L.R. 0.009000\n",
      "Epoch 41/100, T. Loss: 1.0546, V. Loss: 0.9308, Early S: NO improvement in the last 2 epochs, Gamma: 0.9, S. Size: 15, L.R. 0.008100\n",
      "Epoch 61/100, T. Loss: 1.0461, V. Loss: 0.9642, Early S: NO improvement in the last 6 epochs, Gamma: 0.9, S. Size: 15, L.R. 0.006561\n",
      "Epoch 81/100, T. Loss: 1.0403, V. Loss: 0.9222, Early S: NO improvement in the last 26 epochs, Gamma: 0.9, S. Size: 15, L.R. 0.005905\n",
      "Epoch 100/100, T. Loss: 1.0316, V. Loss: 0.9327, Early S: NO improvement in the last 18 epochs, Gamma: 0.9, S. Size: 15, L.R. 0.005314\n",
      "Training completed in 1137.86 seconds\n",
      "Training mode: LR Scheduler\n",
      "Epoch 1/100, T. Loss: 1.2865, V. Loss: 1.0384, Early S: , Gamma: 0.9, S. Size: 10, L.R. 0.010000\n",
      "Epoch 21/100, T. Loss: 1.0675, V. Loss: 0.9396, Early S: NO improvement in the last 6 epochs, Gamma: 0.9, S. Size: 10, L.R. 0.008100\n",
      "Epoch 41/100, T. Loss: 1.0483, V. Loss: 0.9592, Early S: NO improvement in the last 4 epochs, Gamma: 0.9, S. Size: 10, L.R. 0.006561\n",
      "Epoch 61/100, T. Loss: 1.0447, V. Loss: 0.9401, Early S: NO improvement in the last 17 epochs, Gamma: 0.9, S. Size: 10, L.R. 0.005314\n",
      "Epoch 81/100, T. Loss: 1.0373, V. Loss: 0.9294, Early S: NO improvement in the last 11 epochs, Gamma: 0.9, S. Size: 10, L.R. 0.004305\n",
      "Epoch 100/100, T. Loss: 1.0377, V. Loss: 0.9379, Early S: NO improvement in the last 30 epochs, Gamma: 0.9, S. Size: 10, L.R. 0.003874\n",
      "Training completed in 1515.06 seconds\n",
      "Training mode: LR Scheduler\n",
      "Epoch 1/100, T. Loss: 1.2902, V. Loss: 1.0531, Early S: , Gamma: 0.9, S. Size: 5, L.R. 0.010000\n",
      "Epoch 21/100, T. Loss: 1.0652, V. Loss: 0.9927, Early S: Improvement!!!, actual counter 4, Gamma: 0.9, S. Size: 5, L.R. 0.006561\n",
      "Epoch 41/100, T. Loss: 1.0471, V. Loss: 0.9369, Early S: NO improvement in the last 2 epochs, Gamma: 0.9, S. Size: 5, L.R. 0.004305\n",
      "Epoch 61/100, T. Loss: 1.0349, V. Loss: 0.9367, Early S: NO improvement in the last 8 epochs, Gamma: 0.9, S. Size: 5, L.R. 0.002824\n",
      "Epoch 81/100, T. Loss: 1.0358, V. Loss: 0.9553, Early S: NO improvement in the last 28 epochs, Gamma: 0.9, S. Size: 5, L.R. 0.001853\n",
      "\n",
      "Training completed in 1865.15 seconds\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "61b39c04e855a8cc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
