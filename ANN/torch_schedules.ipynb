{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Training schedules for pytorch\n",
    "Learning rate schedules are mechanisms used during the training of neural networks to adjust the learning rate over time. They are designed to decrease the learning rate as the training progresses, allowing the network to make large adjustments in the initial stages of training, when the weights a likely far from their optimal values, and then make smaller adjustments as the training progresses, to fine-tune the weights. These adjustments helps mitigate the risk of overshooting the minimum point of the loss function and helps to reach convergence more smoothly.\n",
    "\n",
    "In PyTorch, one of the learning rate scheduling tools is the StepLR class, found in the `torch.optim.lr_scheduler` module. StepLR is a type of learning rate schedule that decreases the learning rate by a certain factor every few epochs. This allows the learning rate to decrease in a step-wise fashion rather than continuously, which can be beneficial in some cases, as it gives the model time to 'settle' into ares of the loss landscape before the learning rate is reduced further.\n",
    "\n",
    "StepLR takes three parameters:\n",
    "\n",
    "- optimizer: the optimizer you are using to train your model\n",
    "- step_size: this is the number of epochs after which you want to reduce the learning rate. For instance, if step_size=10, then the learning rate will be reduced every 10 epochs.\n",
    "- gamma: this is the factor by which the learning rate will be reduced at each step. For instance, if gamma = 0.1, the learning rate will be multiplied by 0.1 at each step; this is a reduction of lr by 90%\n",
    "\n",
    "The StepLR scheduler is used during the training loop. After each step of the optimizer (after `optimizer.step()`), you call `scheduler.step()` to adjust the learning rate according to the schedule.\n",
    "\n",
    "It's worth nothing that the choice of step_size and gamma can be important, and may need to be tuned based on your specific problem and dataset. Too large step_size and the learning rate may not reduce quickly enough; too small and it may reduce too quickly. Similarly, a gamma too close to 1 may not reduce the learning rate significantly enough, while a gamma too small may reduce it too quickly.\n",
    "\n",
    "### Schedule changing the lr with k-fold cross validation"
   ],
   "id": "7b040caa3dec799c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-31T23:20:07.897129Z",
     "start_time": "2024-07-31T23:20:00.650969Z"
    }
   },
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T23:20:22.137790Z",
     "start_time": "2024-07-31T23:20:22.110798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# selecting device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# early stopping definition\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_model = None\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.status = \"\"\n",
    "        \n",
    "    def __call__(self, model, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "        elif self.best_loss - val_loss >= self.min_delta:\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.status = f\"Improvement found, counter reset to {self.counter}\"\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            self.status = f\"No improvement found in the last {self.counter} epochs\"\n",
    "            if self.counter >= self.patience:\n",
    "                self.status = f\"Early stopping triggered after {self.counter} epochs\"\n",
    "                if self.restore_best_weights:\n",
    "                    model.load_state_dict(self.best_model)\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "print(device)"
   ],
   "id": "e93fd897d50ff8d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T23:20:51.357067Z",
     "start_time": "2024-07-31T23:20:51.285174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load the data adn fill NA values with the median for all fields\n",
    "def fill_na_with_median(df):\n",
    "    \"\"\"\n",
    "    Replace all na values in numeric fields with median\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['float32', 'int32', 'int64', 'float64', 'long']:\n",
    "            median = df[col].median()\n",
    "            df[col].fillna(median, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_dataset(route, fill_na=False):\n",
    "    df = pd.read_csv(route, na_values=[\"NA\", \"?\"])\n",
    "    if fill_na:\n",
    "      df = fill_na_with_median(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_dataset(\"data/jh-simple-dataset.csv\", fill_na=True)\n",
    "df.info()"
   ],
   "id": "ffa74dcec01d813b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 14 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   id              2000 non-null   int64  \n",
      " 1   job             2000 non-null   object \n",
      " 2   area            2000 non-null   object \n",
      " 3   income          2000 non-null   float64\n",
      " 4   aspect          2000 non-null   float64\n",
      " 5   subscriptions   2000 non-null   int64  \n",
      " 6   dist_healthy    2000 non-null   float64\n",
      " 7   save_rate       2000 non-null   int64  \n",
      " 8   dist_unhealthy  2000 non-null   float64\n",
      " 9   age             2000 non-null   int64  \n",
      " 10  pop_dense       2000 non-null   float64\n",
      " 11  retail_dense    2000 non-null   float64\n",
      " 12  crime           2000 non-null   float64\n",
      " 13  product         2000 non-null   object \n",
      "dtypes: float64(7), int64(4), object(3)\n",
      "memory usage: 218.9+ KB\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T23:23:56.346069Z",
     "start_time": "2024-07-31T23:23:56.315221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generate dummies\n",
    "def generate_dummies(df, dummies_list, drop_original=True):\n",
    "    \"\"\"Generate dummies for certain categorical columns\"\"\"\n",
    "    for col in dummies_list:\n",
    "        if df[col].dtype in ['object', 'categories']:\n",
    "            df = pd.concat([df, pd.get_dummies(df[col], prefix=col, dtype=int)], axis=1)\n",
    "            if drop_original:\n",
    "                df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = generate_dummies(df, [\"job\", \"area\", \"product\"])\n",
    "df.info()"
   ],
   "id": "497e6d940b4bcac6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 55 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   id              2000 non-null   int64  \n",
      " 1   income          2000 non-null   float64\n",
      " 2   aspect          2000 non-null   float64\n",
      " 3   subscriptions   2000 non-null   int64  \n",
      " 4   dist_healthy    2000 non-null   float64\n",
      " 5   save_rate       2000 non-null   int64  \n",
      " 6   dist_unhealthy  2000 non-null   float64\n",
      " 7   age             2000 non-null   int64  \n",
      " 8   pop_dense       2000 non-null   float64\n",
      " 9   retail_dense    2000 non-null   float64\n",
      " 10  crime           2000 non-null   float64\n",
      " 11  job_11          2000 non-null   int32  \n",
      " 12  job_al          2000 non-null   int32  \n",
      " 13  job_am          2000 non-null   int32  \n",
      " 14  job_ax          2000 non-null   int32  \n",
      " 15  job_bf          2000 non-null   int32  \n",
      " 16  job_by          2000 non-null   int32  \n",
      " 17  job_cv          2000 non-null   int32  \n",
      " 18  job_de          2000 non-null   int32  \n",
      " 19  job_dz          2000 non-null   int32  \n",
      " 20  job_e2          2000 non-null   int32  \n",
      " 21  job_f8          2000 non-null   int32  \n",
      " 22  job_gj          2000 non-null   int32  \n",
      " 23  job_gv          2000 non-null   int32  \n",
      " 24  job_kd          2000 non-null   int32  \n",
      " 25  job_ke          2000 non-null   int32  \n",
      " 26  job_kl          2000 non-null   int32  \n",
      " 27  job_kp          2000 non-null   int32  \n",
      " 28  job_ks          2000 non-null   int32  \n",
      " 29  job_kw          2000 non-null   int32  \n",
      " 30  job_mm          2000 non-null   int32  \n",
      " 31  job_nb          2000 non-null   int32  \n",
      " 32  job_nn          2000 non-null   int32  \n",
      " 33  job_ob          2000 non-null   int32  \n",
      " 34  job_pe          2000 non-null   int32  \n",
      " 35  job_po          2000 non-null   int32  \n",
      " 36  job_pq          2000 non-null   int32  \n",
      " 37  job_pz          2000 non-null   int32  \n",
      " 38  job_qp          2000 non-null   int32  \n",
      " 39  job_qw          2000 non-null   int32  \n",
      " 40  job_rn          2000 non-null   int32  \n",
      " 41  job_sa          2000 non-null   int32  \n",
      " 42  job_vv          2000 non-null   int32  \n",
      " 43  job_zz          2000 non-null   int32  \n",
      " 44  area_a          2000 non-null   int32  \n",
      " 45  area_b          2000 non-null   int32  \n",
      " 46  area_c          2000 non-null   int32  \n",
      " 47  area_d          2000 non-null   int32  \n",
      " 48  product_a       2000 non-null   int32  \n",
      " 49  product_b       2000 non-null   int32  \n",
      " 50  product_c       2000 non-null   int32  \n",
      " 51  product_d       2000 non-null   int32  \n",
      " 52  product_e       2000 non-null   int32  \n",
      " 53  product_f       2000 non-null   int32  \n",
      " 54  product_g       2000 non-null   int32  \n",
      "dtypes: float64(7), int32(44), int64(4)\n",
      "memory usage: 515.8 KB\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T23:27:18.763217Z",
     "start_time": "2024-07-31T23:27:18.743099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# standardize ranges\n",
    "num_fields = ['income', 'aspect', 'save_rate', 'subscriptions']\n",
    "for field in num_fields:\n",
    "    df[field] = zscore(df[field])"
   ],
   "id": "f14bde7d90430c7b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# random torch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# features dataset (x)\n",
    "x_columns = df.columns.drop(['age', 'id'])\n",
    "x = torch.tensor(df[x_columns].values, device=device, dtype=torch.float32)\n",
    "\n",
    "# labels dataset (y)\n",
    "y = torch.tensor(df[['age']].values, device=device, dtype=torch.float32)\n",
    "\n",
    "# cross validate\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "# early stopping parameters\n",
    "patience = 10\n",
    "\n",
    "fold = 0\n",
    "for train_index, test_index in kf.split(x):\n",
    "    fold += 1\n",
    "    print(f\"Fold {fold}\")\n",
    "    \n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # pytorch dataloader\n",
    "    "
   ],
   "id": "f63c23e59e08bd98"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
