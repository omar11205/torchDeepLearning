{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-27T03:42:21.660222Z",
     "start_time": "2024-09-27T03:42:17.119316Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T03:42:24.495732Z",
     "start_time": "2024-09-27T03:42:24.427062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# setting device and reproducibility\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42)\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "d2aee46ac428b403",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Classification task\n",
    "In this part of the code I will perform a classification task trying to predict the field 'fare_class'. But i will create more classes (5) in this field. In the original dataset the class is 1 if the 'fare_amount' of each trip is greater than 10 dollars otherwise is 0."
   ],
   "id": "33cb0f2e88f0e3ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T03:42:39.827859Z",
     "start_time": "2024-09-27T03:42:39.657321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import data \n",
    "data_frame = pd.read_csv(\"data/NYCTaxiFares.csv\", na_values=[\"NA\", \"?\"])"
   ],
   "id": "a22b6ffa3e98515e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Preprocessing functions\n",
    "All the feature engineering the extraction of embedding sizes, and the creation of the tensors are resumed in this functions.\n",
    "The function `custom_fare_class` reclassify from a binary (0,1) to n list of classes according to the `fare_amount` feature."
   ],
   "id": "84d423bf96352030"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T03:42:41.353817Z",
     "start_time": "2024-09-27T03:42:41.339855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function to calculate the distance of the travel\n",
    "def haversine_distance(dat_f, lat1, lon1, lat2, lon2):\n",
    "    \n",
    "    # average radius of the Earth in (km)\n",
    "    r = 6371\n",
    "    \n",
    "    phi1 = np.radians(dat_f[lat1])\n",
    "    phi2 = np.radians(dat_f[lat2])\n",
    "    delta_phi = np.radians(dat_f[lat2] - dat_f[lat1])\n",
    "    delta_lambda = np.radians(dat_f[lon2] - dat_f[lon1])\n",
    "    \n",
    "    a = np.sin(delta_phi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    d = (r * c)\n",
    "    \n",
    "    return d\n",
    "\n",
    "def preprocessing(df_n, cat_cols):\n",
    "    \"\"\"\n",
    "    Preprocesses the data and adds pandas categorical fields to a dataframe.\n",
    "    :param df_n: pd.DataFrame\n",
    "    :param cat_cols: list of categorical fields\n",
    "    :return: pd.DataFrame\n",
    "    \"\"\"\n",
    "    # append a 'dist_km' new feature in the dataframe\n",
    "    df_n['dist_km'] = haversine_distance(df_n, 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude')\n",
    "    \n",
    "    # remove outliers\n",
    "    dfd = df_n[(df_n['fare_amount'] != 49.57) & (df_n['fare_amount'] != 45.00)].copy()\n",
    "    \n",
    "    # convert to pd datetime\n",
    "    dfd['pickup_datetime'] = pd.to_datetime(dfd['pickup_datetime'])\n",
    "    \n",
    "    # Correcting pickup_datetime due to daylight savings time (April)\n",
    "    dfd['EDTdate'] = dfd['pickup_datetime'] - pd.Timedelta(hours=4)\n",
    "    \n",
    "    # create new time fields\n",
    "    dfd['Hour'] = dfd['EDTdate'].dt.hour\n",
    "    dfd['AMorPM'] = np.where(dfd['Hour']<12, 'am', 'pm')\n",
    "    dfd['Weekday'] = dfd['EDTdate'].dt.strftime(\"%a\")\n",
    "    \n",
    "    # transform to pandas categorical variables\n",
    "    for cat in cat_cols:\n",
    "        dfd[cat] = dfd[cat].astype('category')\n",
    "    \n",
    "    dfd = dfd.drop(columns=['pickup_datetime'])\n",
    "    \n",
    "    return dfd\n",
    "\n",
    "def custom_fare_class(in_df, bins, labels=False):\n",
    "    \"\"\"\n",
    "    Reclassify 'fare_class' to a custom number of fare classes based on the 'fare_amount' field\n",
    "    :param in_df: pd.DataFrame\n",
    "    :param bins: list of integers specifying the number of bins\n",
    "    :param labels: list of integers specifying the number of labels or False for no labels\n",
    "    :return: pd.DataFrame\n",
    "    \"\"\"\n",
    "    in_df['fare_class'] = pd.cut(in_df['fare_amount'], bins=bins, labels=labels)\n",
    "    return in_df\n",
    "\n",
    "def model_tensors(df, cat_cols, cont_cols, y_col):\n",
    "    \"\"\"\n",
    "    Get categorical, continuous and label tensors for the model\n",
    "    :param df: pd.DataFrame\n",
    "    :param cat_cols: list of categorical fields\n",
    "    :param cont_cols: list of continuous fields\n",
    "    :param y_col: list with the labels\n",
    "    :return: cats, conts, y tensors\n",
    "    \"\"\"\n",
    "    \n",
    "    # group the data in categorical continuous and target label    \n",
    "    cats = np.stack([df[col].cat.codes.values for col in cat_cols], axis=1)\n",
    "    conts = np.stack([df[col].values for col in cont_cols], axis=1)\n",
    "    y = df[y_col].values.reshape(-1, 1)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    cats_t = torch.tensor(cats, dtype=torch.int64)\n",
    "    conts_t = torch.tensor(conts, dtype=torch.float32)\n",
    "    y_t = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    return cats_t, conts_t, y_t\n",
    "\n",
    "def create_embedding_sizes(df, cat_cols):\n",
    "    \"\"\"\n",
    "    Create embedding sizes for PyTorch embedding layers\n",
    "    :param df: pd.DataFrame\n",
    "    :param cat_cols: list of categorical fields\n",
    "    :return: emb_sizes list\n",
    "    \"\"\"\n",
    "    # categorical sizes list\n",
    "    cat_sizes = [len(df[col].cat.categories) for col in cat_cols]\n",
    "\n",
    "    # embedding sizes list (divide the number of unique entries in each column by two, if the result is greater than 50 select 50)\n",
    "    emb_sizes = [(size, min(50,(size+1)//2)) for size in cat_sizes]\n",
    "    \n",
    "    return emb_sizes"
   ],
   "id": "de6293f88c3485d2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T03:42:51.462981Z",
     "start_time": "2024-09-27T03:42:46.271990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = preprocessing(data_frame, ['Hour', 'AMorPM', 'Weekday'])\n",
    "\n",
    "# pass from [0, 1] = 2 categories to [0, 1, 2, 3, 4] = 5 categories\n",
    "df = custom_fare_class(df, bins=[0, 10, 20, 30, 40, 50], labels=False)   \n",
    "\n",
    "# now the y labels tensor must be: 'fare_class' \n",
    "cats, conts, y = model_tensors(df, ['Hour', 'AMorPM', 'Weekday'], ['dist_km', 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'], ['fare_class'])\n",
    "\n",
    "# number of continuous fields of the conts tensor\n",
    "n_cont = conts.shape[1]\n",
    "\n",
    "# neurons for the output layer = number of classes of the label field\n",
    "out_size = len(df['fare_class'].unique())\n",
    "\n",
    "emb_sizes = create_embedding_sizes(df, ['Hour', 'AMorPM', 'Weekday'])"
   ],
   "id": "5ca7730b676e4db0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Model definition\n",
    "The model definition can be the same for the classification task and the regression task"
   ],
   "id": "41b0a266c43237bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T03:42:55.397972Z",
     "start_time": "2024-09-27T03:42:55.387128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the model\n",
    "class TabularModel(nn.Module):\n",
    "    def __init__(self, emb_sizes, n_cont, out_size, layers, p=0.5):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in emb_sizes])\n",
    "        self.emb_drop = nn.Dropout(p)\n",
    "        self.batch_norm_cont = nn.BatchNorm1d(n_cont)\n",
    "\n",
    "        layer_list = []\n",
    "        n_emb = sum([nf for ni, nf in emb_sizes])\n",
    "        n_in = n_emb + n_cont\n",
    "        for i in layers:\n",
    "            layer_list.append(nn.Linear(n_in, i))\n",
    "            layer_list.append(nn.ReLU(inplace=True))\n",
    "            layer_list.append(nn.BatchNorm1d(i))\n",
    "            layer_list.append(nn.Dropout(p))\n",
    "            n_in = i\n",
    "\n",
    "        layer_list.append(nn.Linear(layers[-1], out_size))\n",
    "        self.layers = nn.Sequential(*layer_list)\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        embeddings = [e(x_cat[:, i]) for i, e in enumerate(self.embeds)]\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        x_cont = self.batch_norm_cont(x_cont)\n",
    "        x = torch.cat([x, x_cont], 1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ],
   "id": "6fab3b9f58c966ec",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Split Function\n",
    "Function to split the dataset in 6. `Train data`: cat_train, con_train, y_train; `Test Data`: cat_test, con_test, y_test"
   ],
   "id": "5c841770291ad718"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T03:42:57.079682Z",
     "start_time": "2024-09-27T03:42:57.037052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_train_test(categoricals, continuous, y_train, test_size=0.2):\n",
    "    # Ensure the input arrays have the same number of rows\n",
    "    assert categoricals.shape[0] == continuous.shape[0] == y_train.shape[0], \"Input arrays must have the same number of rows\"\n",
    "\n",
    "    # Combine the data into a single array for splitting\n",
    "    combined = np.hstack((categoricals, continuous, y_train))\n",
    "\n",
    "    # Split and shuffle combined data into train and test sets\n",
    "    train_data, test_data = train_test_split(combined, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Determine the number of categorical and continuous columns\n",
    "    n_cat_cols = categoricals.shape[1]\n",
    "    n_cont_cols = continuous.shape[1]\n",
    "\n",
    "    # Separate the train and test data back into categorical, continuous, and target tensors\n",
    "\n",
    "    # selects all rows and the first n_cat_cols columns (categorical features).\n",
    "    cat_train = torch.tensor(train_data[:, :n_cat_cols], dtype=torch.int64).to(device)\n",
    "\n",
    "    # selects all rows and the columns from n_cat_cols to n_cat_cols + n_cont_cols (continuous features).\n",
    "    con_train = torch.tensor(train_data[:, n_cat_cols:n_cat_cols + n_cont_cols], dtype=torch.float32).to(device)\n",
    "\n",
    "    # selects all rows and the last column (target labels).\n",
    "    y_train = torch.tensor(train_data[:, -1], dtype=torch.long).to(device)\n",
    "\n",
    "    cat_test = torch.tensor(test_data[:, :n_cat_cols], dtype=torch.int64).to(device)\n",
    "    con_test = torch.tensor(test_data[:, n_cat_cols:n_cat_cols + n_cont_cols], dtype=torch.float32).to(device)\n",
    "    y_test = torch.tensor(test_data[:, -1], dtype=torch.long).to(device)\n",
    "\n",
    "    return cat_train, con_train, y_train, cat_test, con_test, y_test\n",
    "\n",
    "cat_train, con_train, y_train, cat_test, con_test, y_test = get_train_test(cats, conts, y)"
   ],
   "id": "c6e2871169fe31d6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training instances",
   "id": "a191641b54837561"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T03:43:00.941397Z",
     "start_time": "2024-09-27T03:42:58.579428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model instance \n",
    "model = TabularModel(emb_sizes, conts.shape[1], out_size, [400, 300, 200, 100], p=0.2).to(device)\n",
    "\n",
    "# criteria\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# TensorDatasets\n",
    "train_dataset = TensorDataset(cat_train, con_train, y_train)\n",
    "test_dataset = TensorDataset(cat_test, con_test, y_test)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# results\n",
    "results = []"
   ],
   "id": "f7b9be9871e12487",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Training Loop",
   "id": "a7e2aef8701b357c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T03:59:44.870451Z",
     "start_time": "2024-09-27T03:43:04.830925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "epochs = 150\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "\n",
    "    for cat_batch, con_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(cat_batch, con_batch)\n",
    "        loss = torch.sqrt(criterion(y_pred, y_batch))\n",
    "        epoch_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_losses.append(np.mean(epoch_losses))\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_val_loss = []\n",
    "        for cat_batch, con_batch, y_batch in test_loader:\n",
    "            y_val = model(cat_batch, con_batch)\n",
    "            val_loss = torch.sqrt(criterion(y_val, y_batch)) \n",
    "            epoch_val_loss.append(val_loss.item())\n",
    "\n",
    "        val_losses.append(np.mean(epoch_val_loss))\n",
    "\n",
    "    if epoch % 25 == 0 or epoch == epochs - 1:\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Validation Loss: {val_losses[-1]:.4f}')\n",
    "    \n",
    "    results.append({\n",
    "        \"Epoch\": epoch + 1,\n",
    "        \"Train Loss\": train_losses[-1],\n",
    "        \"Validation Loss\": val_losses[-1],\n",
    "    })\n",
    "    \n",
    "\n",
    "print(f'Training completed in {time.time() - start_time:.2f} seconds')"
   ],
   "id": "c01a202a0feb29d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150, Train Loss: 0.6993, Validation Loss: 0.6096\n",
      "Epoch 26/150, Train Loss: 0.5644, Validation Loss: 0.5479\n",
      "Epoch 51/150, Train Loss: 0.5551, Validation Loss: 0.5437\n",
      "Epoch 76/150, Train Loss: 0.5482, Validation Loss: 0.5450\n",
      "Epoch 101/150, Train Loss: 0.5435, Validation Loss: 0.5424\n",
      "Epoch 126/150, Train Loss: 0.5389, Validation Loss: 0.5440\n",
      "Epoch 150/150, Train Loss: 0.5359, Validation Loss: 0.5453\n",
      "Training completed in 1000.03 seconds\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4b4600b7df9951d7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
