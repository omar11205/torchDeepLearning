{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-10T01:48:00.545663Z",
     "start_time": "2024-08-10T01:47:51.048421Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import copy\n",
    "# from class_models import early_stop"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T01:48:03.466885Z",
     "start_time": "2024-08-10T01:48:03.431228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# setting device and reproducibility\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42)\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "8b6358891ee8e788",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Learning schedule to find the best learning rate using *ReduceOnPlateau* scheduler\n",
    "The learning rate scheduler (ReduceLROnPlateau) adjusts the learning rate based on validation loss. The use of ReduceLROnPlateau scheduler helps in adjusting the learning rate dynamically based on the validation loss, which can improve the training process. This training task tracks the best model in the k-fold. The best model is tracked by comparing the validation loss of each epoch. The learning rate (best_lr) that resulted in the lowest validation loss is recorded."
   ],
   "id": "e47ae840dd103484"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T01:48:10.819701Z",
     "start_time": "2024-08-10T01:48:10.640505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import data \n",
    "data_frame = pd.read_csv(\"data/NYCTaxiFares.csv\", na_values=[\"NA\", \"?\"])"
   ],
   "id": "5aa4b73fe3e92c46",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T01:48:14.561121Z",
     "start_time": "2024-08-10T01:48:14.544668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function to calculate the distance of the travel\n",
    "def haversine_distance(dat_f, lat1, lon1, lat2, lon2):\n",
    "    \n",
    "    # average radius of the Earth in (km)\n",
    "    r = 6371\n",
    "    \n",
    "    phi1 = np.radians(dat_f[lat1])\n",
    "    phi2 = np.radians(dat_f[lat2])\n",
    "    delta_phi = np.radians(dat_f[lat2] - dat_f[lat1])\n",
    "    delta_lambda = np.radians(dat_f[lon2] - dat_f[lon1])\n",
    "    \n",
    "    a = np.sin(delta_phi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    d = (r * c)\n",
    "    \n",
    "    return d\n",
    "\n",
    "def preprocessing(df_n, cat_cols):\n",
    "    \"\"\"\n",
    "    Preprocesses the data and adds pandas categorical fields to a dataframe.\n",
    "    :param df_n: pandas dataframe \n",
    "    :param cat_cols: list of categorical fields\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    # append a 'dist_km' new feature in the dataframe\n",
    "    df_n['dist_km'] = haversine_distance(df_n, 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude')\n",
    "    \n",
    "    # remove outliers\n",
    "    dfd = df_n[(df_n['fare_amount'] != 49.57) & (df_n['fare_amount'] != 45.00)].copy()\n",
    "    \n",
    "    # convert to pd datetime\n",
    "    dfd['pickup_datetime'] = pd.to_datetime(dfd['pickup_datetime'])\n",
    "    \n",
    "    # Correcting pickup_datetime due to daylight savings time (April)\n",
    "    dfd['EDTdate'] = dfd['pickup_datetime'] - pd.Timedelta(hours=4)\n",
    "    \n",
    "    # create new time fields\n",
    "    dfd['Hour'] = dfd['EDTdate'].dt.hour\n",
    "    dfd['AMorPM'] = np.where(dfd['Hour']<12, 'am', 'pm')\n",
    "    dfd['Weekday'] = dfd['EDTdate'].dt.strftime(\"%a\")\n",
    "    \n",
    "    # transform to pandas categorical variables\n",
    "    for cat in cat_cols:\n",
    "        dfd[cat] = dfd[cat].astype('category')\n",
    "    \n",
    "    dfd = dfd.drop(columns=['pickup_datetime'])\n",
    "    \n",
    "    return dfd\n",
    "\n",
    "def model_tensors(df, cat_cols, cont_cols, y_col):\n",
    "    \"\"\"\n",
    "    Get categorical, continuous and label tensors for the model\n",
    "    :param df: pd dataframe\n",
    "    :param cat_cols: list of categorical fields\n",
    "    :param cont_cols: list of continuous fields\n",
    "    :param y_col: list with the labels\n",
    "    :return: cats, conts, y tensors\n",
    "    \"\"\"\n",
    "    \n",
    "    # group the data in categorical continuous and target label    \n",
    "    cats = np.stack([df[col].cat.codes.values for col in cat_cols], axis=1)\n",
    "    conts = np.stack([df[col].values for col in cont_cols], axis=1)\n",
    "    y = df[y_col].values.reshape(-1, 1)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    cats_t = torch.tensor(cats, dtype=torch.int64).to(device)\n",
    "    conts_t = torch.tensor(conts, dtype=torch.float32).to(device)\n",
    "    y_t = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "    \n",
    "    return cats_t, conts_t, y_t\n",
    "\n",
    "def create_embedding_sizes(df, cat_cols):\n",
    "    \"\"\"\n",
    "    Create embedding sizes for PyTorch embedding layers\n",
    "    :param df: pandas dataframe\n",
    "    :param cat_cols: list of categorical fields\n",
    "    :return: emb_sizes list\n",
    "    \"\"\"\n",
    "    # categorical sizes list\n",
    "    cat_sizes = [len(df[col].cat.categories) for col in cat_cols]\n",
    "\n",
    "    # embedding sizes list (divide the number of unique entries in each column by two, if the result is greater than 50 select 50)\n",
    "    emb_sizes = [(size, min(50,(size+1)//2)) for size in cat_sizes]\n",
    "    \n",
    "    return emb_sizes\n",
    "\n"
   ],
   "id": "9b52fe568f07205e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T01:48:28.700185Z",
     "start_time": "2024-08-10T01:48:24.169960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = preprocessing(data_frame, ['Hour', 'AMorPM', 'Weekday'])\n",
    "\n",
    "cats, conts, y = model_tensors(df, ['Hour', 'AMorPM', 'Weekday'], ['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude', 'dist_km'], ['fare_amount'])\n",
    "\n",
    "# number of continuous fields of the conts tensor\n",
    "n_cont = conts.shape[1]\n",
    "\n",
    "emb_sizes = create_embedding_sizes(df, ['Hour', 'AMorPM', 'Weekday'])"
   ],
   "id": "6415281a4de6470",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model definition",
   "id": "fe20a455c5e22d44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T01:48:33.737934Z",
     "start_time": "2024-08-10T01:48:33.723713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the model\n",
    "class TabularModel(nn.Module):\n",
    "    def __init__(self, emb_sizes, n_cont, out_size, layers, p=0.5):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in emb_sizes])\n",
    "        self.emb_drop = nn.Dropout(p)\n",
    "        self.batch_norm_cont = nn.BatchNorm1d(n_cont)\n",
    "\n",
    "        layer_list = []\n",
    "        n_emb = sum([nf for ni, nf in emb_sizes])\n",
    "        n_in = n_emb + n_cont\n",
    "        for i in layers:\n",
    "            layer_list.append(nn.Linear(n_in, i))\n",
    "            layer_list.append(nn.ReLU(inplace=True))\n",
    "            layer_list.append(nn.BatchNorm1d(i))\n",
    "            layer_list.append(nn.Dropout(p))\n",
    "            n_in = i\n",
    "\n",
    "        layer_list.append(nn.Linear(layers[-1], out_size))\n",
    "        self.layers = nn.Sequential(*layer_list)\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        embeddings = [e(x_cat[:, i]) for i, e in enumerate(self.embeds)]\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        x_cont = self.batch_norm_cont(x_cont)\n",
    "        x = torch.cat([x, x_cont], 1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ],
   "id": "e7bc0aa7a2948d56",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### K-folds function generator\n",
    "Since the *train_index* and the *val_index* are those who effectively creates the training subsets we can create all the six subsets: 2 for categorical, 2 for continuous and 2 for y labels."
   ],
   "id": "716c3d6415d5e47b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T01:48:36.949140Z",
     "start_time": "2024-08-10T01:48:36.938351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function-generator to split the data into k folds\n",
    "def kfold_split(k, X_cat, X_cont, y):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    for train_index, val_index in kf.split(X_cat):\n",
    "        X_cat_train, X_cat_val = X_cat[train_index], X_cat[val_index]\n",
    "        X_cont_train, X_cont_val = X_cont[train_index], X_cont[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        yield X_cat_train, X_cat_val, X_cont_train, X_cont_val, y_train, y_val"
   ],
   "id": "78fbc603770e3584",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model and training parameters and hyperparameters ",
   "id": "3e7f5652ef3b64d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T01:48:49.829160Z",
     "start_time": "2024-08-10T01:48:49.822821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# emb_sizes\n",
    "# cont\n",
    "out_size = 1 # <- for regression task\n",
    "layers = [200, 100]\n",
    "p = 0.2 # dropout probability\n",
    "batch_size = 1024\n",
    "epochs = 100"
   ],
   "id": "a6ac861eeaadc06e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cross-Validation loop",
   "id": "fcdb5be4d2efddbc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T01:55:13.512462Z",
     "start_time": "2024-08-10T01:48:58.795143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cross-validation loop\n",
    "k = 12\n",
    "fold = 0\n",
    "best_lr = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "for X_cat_train, X_cat_val, X_cont_train, X_cont_val, y_train, y_val in kfold_split(k, cats, conts, y):\n",
    "    fold += 1\n",
    "    print(f\"Fold {fold}/{k}\")\n",
    "\n",
    "    # Create datasets and loaders\n",
    "    train_dataset = TensorDataset(X_cat_train, X_cont_train, y_train)\n",
    "    val_dataset = TensorDataset(X_cat_val, X_cont_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize model, loss, optimizer, and scheduler\n",
    "    model = TabularModel(emb_sizes, n_cont, out_size, layers, p).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        for X_cat_batch, X_cont_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_cat_batch, X_cont_batch)\n",
    "            loss = torch.sqrt(criterion(y_pred, y_batch))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for X_cat_batch, X_cont_batch, y_batch in val_loader:\n",
    "                y_pred = model(X_cat_batch, X_cont_batch)\n",
    "                loss = torch.sqrt(criterion(y_pred, y_batch))\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        mean_val_loss = np.mean(val_losses)\n",
    "        scheduler.step(mean_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {np.mean(train_losses):.4f}, Val Loss: {mean_val_loss:.4f}\")\n",
    "\n",
    "        # Check for best model\n",
    "        if mean_val_loss < best_loss:\n",
    "            best_loss = mean_val_loss\n",
    "            best_lr = optimizer.param_groups[0]['lr']\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "print(f\"Best Learning Rate: {best_lr}\")"
   ],
   "id": "19d52c6a0d1d495b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/12\n",
      "Epoch 1/100, Train Loss: 9.5076, Val Loss: 8.2350\n",
      "Epoch 2/100, Train Loss: 6.1683, Val Loss: 3.3245\n",
      "Epoch 3/100, Train Loss: 3.1408, Val Loss: 2.5852\n",
      "Epoch 4/100, Train Loss: 3.0029, Val Loss: 2.5468\n",
      "Epoch 5/100, Train Loss: 2.9456, Val Loss: 2.5598\n",
      "Epoch 6/100, Train Loss: 2.8658, Val Loss: 2.5057\n",
      "Epoch 7/100, Train Loss: 2.8160, Val Loss: 2.5120\n",
      "Epoch 8/100, Train Loss: 2.8126, Val Loss: 2.4971\n",
      "Epoch 9/100, Train Loss: 2.7759, Val Loss: 2.4944\n",
      "Epoch 10/100, Train Loss: 2.7595, Val Loss: 2.4792\n",
      "Epoch 11/100, Train Loss: 2.7406, Val Loss: 2.4781\n",
      "Epoch 12/100, Train Loss: 2.7246, Val Loss: 2.4734\n",
      "Epoch 13/100, Train Loss: 2.7339, Val Loss: 2.4679\n",
      "Epoch 14/100, Train Loss: 2.6977, Val Loss: 2.4689\n",
      "Epoch 15/100, Train Loss: 2.6869, Val Loss: 2.4613\n",
      "Epoch 16/100, Train Loss: 2.6692, Val Loss: 2.4599\n",
      "Epoch 17/100, Train Loss: 2.6694, Val Loss: 2.4553\n",
      "Epoch 18/100, Train Loss: 2.6657, Val Loss: 2.4588\n",
      "Epoch 19/100, Train Loss: 2.6567, Val Loss: 2.4518\n",
      "Epoch 20/100, Train Loss: 2.6564, Val Loss: 2.4444\n",
      "Epoch 21/100, Train Loss: 2.6311, Val Loss: 2.4409\n",
      "Epoch 22/100, Train Loss: 2.6301, Val Loss: 2.4436\n",
      "Epoch 23/100, Train Loss: 2.6337, Val Loss: 2.4365\n",
      "Epoch 24/100, Train Loss: 2.6232, Val Loss: 2.4353\n",
      "Epoch 25/100, Train Loss: 2.6296, Val Loss: 2.4377\n",
      "Epoch 26/100, Train Loss: 2.6228, Val Loss: 2.4453\n",
      "Epoch 27/100, Train Loss: 2.6093, Val Loss: 2.4248\n",
      "Epoch 28/100, Train Loss: 2.6024, Val Loss: 2.4282\n",
      "Epoch 29/100, Train Loss: 2.5934, Val Loss: 2.4287\n",
      "Epoch 30/100, Train Loss: 2.6011, Val Loss: 2.4164\n",
      "Epoch 31/100, Train Loss: 2.6116, Val Loss: 2.4180\n",
      "Epoch 32/100, Train Loss: 2.6014, Val Loss: 2.4240\n",
      "Epoch 33/100, Train Loss: 2.5803, Val Loss: 2.4176\n",
      "Epoch 34/100, Train Loss: 2.5839, Val Loss: 2.4200\n",
      "Epoch 35/100, Train Loss: 2.5768, Val Loss: 2.4105\n",
      "Epoch 36/100, Train Loss: 2.5742, Val Loss: 2.4164\n",
      "Epoch 37/100, Train Loss: 2.5828, Val Loss: 2.4057\n",
      "Epoch 38/100, Train Loss: 2.5819, Val Loss: 2.4068\n",
      "Epoch 39/100, Train Loss: 2.5772, Val Loss: 2.4010\n",
      "Epoch 40/100, Train Loss: 2.5701, Val Loss: 2.4118\n",
      "Epoch 41/100, Train Loss: 2.5691, Val Loss: 2.4003\n",
      "Epoch 42/100, Train Loss: 2.5630, Val Loss: 2.3935\n",
      "Epoch 43/100, Train Loss: 2.5661, Val Loss: 2.4024\n",
      "Epoch 44/100, Train Loss: 2.5510, Val Loss: 2.3977\n",
      "Epoch 45/100, Train Loss: 2.5714, Val Loss: 2.4048\n",
      "Epoch 46/100, Train Loss: 2.5635, Val Loss: 2.3961\n",
      "Epoch 47/100, Train Loss: 2.5550, Val Loss: 2.3998\n",
      "Epoch 48/100, Train Loss: 2.5529, Val Loss: 2.3917\n",
      "Epoch 49/100, Train Loss: 2.5608, Val Loss: 2.4004\n",
      "Epoch 50/100, Train Loss: 2.5584, Val Loss: 2.3899\n",
      "Epoch 51/100, Train Loss: 2.5369, Val Loss: 2.3884\n",
      "Epoch 52/100, Train Loss: 2.5647, Val Loss: 2.3948\n",
      "Epoch 53/100, Train Loss: 2.5431, Val Loss: 2.3888\n",
      "Epoch 54/100, Train Loss: 2.5430, Val Loss: 2.3853\n",
      "Epoch 55/100, Train Loss: 2.5429, Val Loss: 2.3945\n",
      "Epoch 56/100, Train Loss: 2.5459, Val Loss: 2.3793\n",
      "Epoch 57/100, Train Loss: 2.5444, Val Loss: 2.3822\n",
      "Epoch 58/100, Train Loss: 2.5484, Val Loss: 2.3923\n",
      "Epoch 59/100, Train Loss: 2.5382, Val Loss: 2.3935\n",
      "Epoch 60/100, Train Loss: 2.5493, Val Loss: 2.3842\n",
      "Epoch 61/100, Train Loss: 2.5454, Val Loss: 2.3966\n",
      "Epoch 62/100, Train Loss: 2.5391, Val Loss: 2.3929\n",
      "Epoch 63/100, Train Loss: 2.5381, Val Loss: 2.3956\n",
      "Epoch 64/100, Train Loss: 2.5289, Val Loss: 2.3863\n",
      "Epoch 65/100, Train Loss: 2.5278, Val Loss: 2.3869\n",
      "Epoch 66/100, Train Loss: 2.5448, Val Loss: 2.3737\n",
      "Epoch 67/100, Train Loss: 2.5253, Val Loss: 2.3833\n",
      "Epoch 68/100, Train Loss: 2.5237, Val Loss: 2.3782\n",
      "Epoch 69/100, Train Loss: 2.5227, Val Loss: 2.3799\n",
      "Epoch 70/100, Train Loss: 2.5102, Val Loss: 2.3898\n",
      "Epoch 71/100, Train Loss: 2.5152, Val Loss: 2.3960\n",
      "Epoch 72/100, Train Loss: 2.5277, Val Loss: 2.3809\n",
      "Epoch 73/100, Train Loss: 2.5043, Val Loss: 2.3859\n",
      "Epoch 74/100, Train Loss: 2.5248, Val Loss: 2.3703\n",
      "Epoch 75/100, Train Loss: 2.5250, Val Loss: 2.3853\n",
      "Epoch 76/100, Train Loss: 2.5201, Val Loss: 2.3814\n",
      "Epoch 77/100, Train Loss: 2.5197, Val Loss: 2.3702\n",
      "Epoch 78/100, Train Loss: 2.5192, Val Loss: 2.3767\n",
      "Epoch 79/100, Train Loss: 2.5144, Val Loss: 2.3772\n",
      "Epoch 80/100, Train Loss: 2.5197, Val Loss: 2.3669\n",
      "Epoch 81/100, Train Loss: 2.5108, Val Loss: 2.3794\n",
      "Epoch 82/100, Train Loss: 2.5038, Val Loss: 2.3670\n",
      "Epoch 83/100, Train Loss: 2.5122, Val Loss: 2.3731\n",
      "Epoch 84/100, Train Loss: 2.5177, Val Loss: 2.3849\n",
      "Epoch 85/100, Train Loss: 2.5173, Val Loss: 2.3620\n",
      "Epoch 86/100, Train Loss: 2.5360, Val Loss: 2.3793\n",
      "Epoch 87/100, Train Loss: 2.5197, Val Loss: 2.3672\n",
      "Epoch 88/100, Train Loss: 2.5127, Val Loss: 2.3801\n",
      "Epoch 89/100, Train Loss: 2.5233, Val Loss: 2.3849\n",
      "Epoch 90/100, Train Loss: 2.5220, Val Loss: 2.3776\n",
      "Epoch 91/100, Train Loss: 2.5065, Val Loss: 2.3723\n",
      "Epoch 92/100, Train Loss: 2.5117, Val Loss: 2.3750\n",
      "Epoch 93/100, Train Loss: 2.5106, Val Loss: 2.3691\n",
      "Epoch 94/100, Train Loss: 2.5076, Val Loss: 2.3670\n",
      "Epoch 95/100, Train Loss: 2.5030, Val Loss: 2.3722\n",
      "Epoch 96/100, Train Loss: 2.4988, Val Loss: 2.3693\n",
      "Epoch 97/100, Train Loss: 2.5027, Val Loss: 2.3661\n",
      "Epoch 98/100, Train Loss: 2.5002, Val Loss: 2.3561\n",
      "Epoch 99/100, Train Loss: 2.4948, Val Loss: 2.3598\n",
      "Epoch 100/100, Train Loss: 2.4901, Val Loss: 2.3587\n",
      "Fold 2/12\n",
      "Epoch 1/100, Train Loss: 9.6586, Val Loss: 8.7233\n",
      "Epoch 2/100, Train Loss: 6.3749, Val Loss: 3.3854\n",
      "Epoch 3/100, Train Loss: 3.1804, Val Loss: 2.5698\n",
      "Epoch 4/100, Train Loss: 3.0149, Val Loss: 2.5338\n",
      "Epoch 5/100, Train Loss: 2.9248, Val Loss: 2.5382\n",
      "Epoch 6/100, Train Loss: 2.8838, Val Loss: 2.5215\n",
      "Epoch 7/100, Train Loss: 2.8266, Val Loss: 2.4834\n",
      "Epoch 8/100, Train Loss: 2.8087, Val Loss: 2.4995\n",
      "Epoch 9/100, Train Loss: 2.7971, Val Loss: 2.4809\n",
      "Epoch 10/100, Train Loss: 2.7675, Val Loss: 2.4800\n",
      "Epoch 11/100, Train Loss: 2.7622, Val Loss: 2.5076\n",
      "Epoch 12/100, Train Loss: 2.7344, Val Loss: 2.4759\n",
      "Epoch 13/100, Train Loss: 2.6977, Val Loss: 2.4589\n",
      "Epoch 14/100, Train Loss: 2.7270, Val Loss: 2.4417\n",
      "Epoch 15/100, Train Loss: 2.6856, Val Loss: 2.4531\n",
      "Epoch 16/100, Train Loss: 2.6801, Val Loss: 2.4411\n",
      "Epoch 17/100, Train Loss: 2.6799, Val Loss: 2.4419\n",
      "Epoch 18/100, Train Loss: 2.6712, Val Loss: 2.4430\n",
      "Epoch 19/100, Train Loss: 2.6539, Val Loss: 2.4415\n",
      "Epoch 20/100, Train Loss: 2.6554, Val Loss: 2.4284\n",
      "Epoch 21/100, Train Loss: 2.6441, Val Loss: 2.4342\n",
      "Epoch 22/100, Train Loss: 2.6533, Val Loss: 2.5193\n",
      "Epoch 23/100, Train Loss: 2.6453, Val Loss: 2.4249\n",
      "Epoch 24/100, Train Loss: 2.6438, Val Loss: 2.4729\n",
      "Epoch 25/100, Train Loss: 2.6310, Val Loss: 2.4371\n",
      "Epoch 26/100, Train Loss: 2.6269, Val Loss: 2.4569\n",
      "Epoch 27/100, Train Loss: 2.6248, Val Loss: 2.4127\n",
      "Epoch 28/100, Train Loss: 2.6162, Val Loss: 2.4180\n",
      "Epoch 29/100, Train Loss: 2.6153, Val Loss: 2.4000\n",
      "Epoch 30/100, Train Loss: 2.6076, Val Loss: 2.3908\n",
      "Epoch 31/100, Train Loss: 2.6007, Val Loss: 2.3980\n",
      "Epoch 32/100, Train Loss: 2.5977, Val Loss: 2.4041\n",
      "Epoch 33/100, Train Loss: 2.5984, Val Loss: 2.3939\n",
      "Epoch 34/100, Train Loss: 2.5980, Val Loss: 2.3960\n",
      "Epoch 35/100, Train Loss: 2.5865, Val Loss: 2.3864\n",
      "Epoch 36/100, Train Loss: 2.5799, Val Loss: 2.3914\n",
      "Epoch 37/100, Train Loss: 2.5772, Val Loss: 2.3815\n",
      "Epoch 38/100, Train Loss: 2.5862, Val Loss: 2.3872\n",
      "Epoch 39/100, Train Loss: 2.5824, Val Loss: 2.3835\n",
      "Epoch 40/100, Train Loss: 2.5831, Val Loss: 2.4088\n",
      "Epoch 41/100, Train Loss: 2.5802, Val Loss: 2.3791\n",
      "Epoch 42/100, Train Loss: 2.5750, Val Loss: 2.3828\n",
      "Epoch 43/100, Train Loss: 2.5816, Val Loss: 2.3753\n",
      "Epoch 44/100, Train Loss: 2.5659, Val Loss: 2.3808\n",
      "Epoch 45/100, Train Loss: 2.5620, Val Loss: 2.3835\n",
      "Epoch 46/100, Train Loss: 2.5810, Val Loss: 2.3885\n",
      "Epoch 47/100, Train Loss: 2.5860, Val Loss: 2.3844\n",
      "Epoch 48/100, Train Loss: 2.5496, Val Loss: 2.3729\n",
      "Epoch 49/100, Train Loss: 2.5703, Val Loss: 2.3799\n",
      "Epoch 50/100, Train Loss: 2.5534, Val Loss: 2.3688\n",
      "Epoch 51/100, Train Loss: 2.5531, Val Loss: 2.3830\n",
      "Epoch 52/100, Train Loss: 2.5584, Val Loss: 2.3668\n",
      "Epoch 53/100, Train Loss: 2.5585, Val Loss: 2.3822\n",
      "Epoch 54/100, Train Loss: 2.5544, Val Loss: 2.3662\n",
      "Epoch 55/100, Train Loss: 2.5507, Val Loss: 2.3816\n",
      "Epoch 56/100, Train Loss: 2.5361, Val Loss: 2.3705\n",
      "Epoch 57/100, Train Loss: 2.5409, Val Loss: 2.3669\n",
      "Epoch 58/100, Train Loss: 2.5490, Val Loss: 2.3849\n",
      "Epoch 59/100, Train Loss: 2.5552, Val Loss: 2.3684\n",
      "Epoch 60/100, Train Loss: 2.5446, Val Loss: 2.3694\n",
      "Epoch 61/100, Train Loss: 2.5358, Val Loss: 2.3745\n",
      "Epoch 62/100, Train Loss: 2.5400, Val Loss: 2.3663\n",
      "Epoch 63/100, Train Loss: 2.5497, Val Loss: 2.3827\n",
      "Epoch 64/100, Train Loss: 2.5559, Val Loss: 2.3645\n",
      "Epoch 65/100, Train Loss: 2.5255, Val Loss: 2.3732\n",
      "Epoch 66/100, Train Loss: 2.5518, Val Loss: 2.3516\n",
      "Epoch 67/100, Train Loss: 2.5374, Val Loss: 2.3672\n",
      "Epoch 68/100, Train Loss: 2.5449, Val Loss: 2.3599\n",
      "Epoch 69/100, Train Loss: 2.5578, Val Loss: 2.3581\n",
      "Epoch 70/100, Train Loss: 2.5259, Val Loss: 2.3655\n",
      "Epoch 71/100, Train Loss: 2.5318, Val Loss: 2.3697\n",
      "Epoch 72/100, Train Loss: 2.5337, Val Loss: 2.3689\n",
      "Epoch 73/100, Train Loss: 2.5250, Val Loss: 2.3605\n",
      "Epoch 74/100, Train Loss: 2.5341, Val Loss: 2.3702\n",
      "Epoch 75/100, Train Loss: 2.5184, Val Loss: 2.3463\n",
      "Epoch 76/100, Train Loss: 2.5570, Val Loss: 2.3618\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 28\u001B[0m\n\u001B[0;32m     25\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m     26\u001B[0m train_losses \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m---> 28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m X_cat_batch, X_cont_batch, y_batch \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[0;32m     29\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     30\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m model(X_cat_batch, X_cont_batch)\n",
      "File \u001B[1;32mC:\\ANACONDA\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mC:\\ANACONDA\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mC:\\ANACONDA\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcollate_fn(data)\n",
      "File \u001B[1;32mC:\\ANACONDA\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:277\u001B[0m, in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m    216\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[0;32m    217\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    218\u001B[0m \u001B[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001B[39;00m\n\u001B[0;32m    219\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[0;32m    276\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 277\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m collate(batch, collate_fn_map\u001B[38;5;241m=\u001B[39mdefault_collate_fn_map)\n",
      "File \u001B[1;32mC:\\ANACONDA\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28mlen\u001B[39m(elem) \u001B[38;5;241m==\u001B[39m elem_size \u001B[38;5;28;01mfor\u001B[39;00m elem \u001B[38;5;129;01min\u001B[39;00m it):\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124meach element in list of batch should be of equal size\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 141\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    144\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T01:55:17.885204Z",
     "start_time": "2024-08-10T01:55:17.867441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to reset weights and biases\n",
    "def reset_weights(m):\n",
    "    if hasattr(m, 'reset_parameters'):\n",
    "        m.reset_parameters()\n",
    "\n",
    "# Apply the reset function to each layer\n",
    "model.apply(reset_weights)"
   ],
   "id": "4c0afe8311e852c4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularModel(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(24, 12)\n",
       "    (1): Embedding(2, 1)\n",
       "    (2): Embedding(7, 4)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.2, inplace=False)\n",
       "  (batch_norm_cont): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=22, out_features=200, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): Linear(in_features=100, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Learning schedule changing the network architecture, the drop-off probability and applying adaptively learning rate",
   "id": "61ab1e4870a154c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T01:55:22.101307Z",
     "start_time": "2024-08-10T01:55:22.091326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# emb_sizes\n",
    "# cont\n",
    "out_size = 1\n",
    "\n",
    "# Experiment with different architectures and dropout rates\n",
    "arch_configs = [\n",
    "    ([50, 25], 0.1),\n",
    "    ([50, 25], 0.2),\n",
    "    ([50, 25], 0.3),\n",
    "    ([50, 25], 0.4),\n",
    "    ([100, 50], 0.1),\n",
    "    ([100, 50], 0.2),\n",
    "    ([100, 50], 0.3),\n",
    "    ([100, 50], 0.4),\n",
    "    ([200, 100], 0.1),\n",
    "    ([200, 100], 0.2),\n",
    "    ([200, 100], 0.3),\n",
    "    ([200, 100], 0.4),\n",
    "    ([200, 100, 50], 0.1),\n",
    "    ([200, 100, 50], 0.2),\n",
    "    ([200, 100, 50], 0.3),\n",
    "    ([200, 100, 50], 0.4),\n",
    "]\n",
    "\n",
    "arch_configs_r = arch_configs[::-1] # <- inverse the architecture configurations list \n",
    "\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "k = 12"
   ],
   "id": "88e9faaf8e9e7724",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T02:02:32.923854Z",
     "start_time": "2024-08-10T01:55:33.306083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for arch, p in arch_configs_r:\n",
    "    fold = 0\n",
    "    best_lr = None\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    print(f\"Testing architecture: {arch} with dropout: {p}\")\n",
    "\n",
    "    for X_cat_train, X_cat_val, X_cont_train, X_cont_val, y_train, y_val in kfold_split(k, cats, conts, y):\n",
    "        fold += 1\n",
    "        print(f\"Fold {fold}/{k}\")\n",
    "\n",
    "        # Create datasets and loaders\n",
    "        train_dataset = TensorDataset(X_cat_train, X_cont_train, y_train)\n",
    "        val_dataset = TensorDataset(X_cat_val, X_cont_val, y_val)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers = 2)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers = 2)\n",
    "\n",
    "        # Initialize model, loss, optimizer, and scheduler\n",
    "        model = TabularModel(emb_sizes, n_cont, out_size, arch, p).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            train_losses = []\n",
    "\n",
    "            for X_cat_batch, X_cont_batch, y_batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(X_cat_batch, X_cont_batch)\n",
    "                loss = torch.sqrt(criterion(y_pred, y_batch))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            with torch.no_grad():\n",
    "                for X_cat_batch, X_cont_batch, y_batch in val_loader:\n",
    "                    y_pred = model(X_cat_batch, X_cont_batch)\n",
    "                    loss = torch.sqrt(criterion(y_pred, y_batch))\n",
    "                    val_losses.append(loss.item())\n",
    "\n",
    "            mean_val_loss = np.mean(val_losses)\n",
    "            scheduler.step(mean_val_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {np.mean(train_losses):.4f}, Val Loss: {mean_val_loss:.4f}\")\n",
    "\n",
    "            # Check for best model\n",
    "            if mean_val_loss < best_loss:\n",
    "                best_loss = mean_val_loss\n",
    "                best_lr = optimizer.param_groups[0]['lr']\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # Save results for the current fold and configuration\n",
    "        results.append({\n",
    "            \"Architecture\": str(arch),\n",
    "            \"Dropout\": p,\n",
    "            \"Fold\": fold,\n",
    "            \"Best Learning Rate\": best_lr,\n",
    "            \"Best Validation Loss\": best_loss\n",
    "        })"
   ],
   "id": "5cc0cd1c9a1966aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing architecture: [200, 100, 50] with dropout: 0.4\n",
      "Fold 1/12\n",
      "Epoch 1/30, Train Loss: 3.3984, Val Loss: 2.5962\n",
      "Epoch 2/30, Train Loss: 3.0996, Val Loss: 2.5279\n",
      "Epoch 3/30, Train Loss: 3.0242, Val Loss: 2.4725\n",
      "Epoch 4/30, Train Loss: 2.9639, Val Loss: 2.6169\n",
      "Epoch 5/30, Train Loss: 2.9658, Val Loss: 2.4722\n",
      "Epoch 6/30, Train Loss: 2.9258, Val Loss: 2.7005\n",
      "Epoch 7/30, Train Loss: 2.9372, Val Loss: 2.4383\n",
      "Epoch 8/30, Train Loss: 2.9254, Val Loss: 2.3847\n",
      "Epoch 9/30, Train Loss: 2.9132, Val Loss: 2.4316\n",
      "Epoch 10/30, Train Loss: 2.8808, Val Loss: 2.5447\n",
      "Epoch 11/30, Train Loss: 2.8919, Val Loss: 2.4488\n",
      "Epoch 12/30, Train Loss: 2.8792, Val Loss: 2.4315\n",
      "Epoch 13/30, Train Loss: 2.8760, Val Loss: 2.3935\n",
      "Epoch 14/30, Train Loss: 2.8937, Val Loss: 2.4107\n",
      "Epoch 15/30, Train Loss: 2.8730, Val Loss: 2.5446\n",
      "Epoch 16/30, Train Loss: 2.8689, Val Loss: 2.3736\n",
      "Epoch 17/30, Train Loss: 2.8637, Val Loss: 2.5704\n",
      "Epoch 18/30, Train Loss: 2.8487, Val Loss: 2.3565\n",
      "Epoch 19/30, Train Loss: 2.8775, Val Loss: 2.4202\n",
      "Epoch 20/30, Train Loss: 2.8363, Val Loss: 2.3550\n",
      "Epoch 21/30, Train Loss: 2.8493, Val Loss: 2.4680\n",
      "Epoch 22/30, Train Loss: 2.8578, Val Loss: 2.5057\n",
      "Epoch 23/30, Train Loss: 2.8437, Val Loss: 2.5450\n",
      "Epoch 24/30, Train Loss: 2.8448, Val Loss: 2.4329\n",
      "Epoch 25/30, Train Loss: 2.8345, Val Loss: 2.4012\n",
      "Epoch 26/30, Train Loss: 2.8501, Val Loss: 2.4312\n",
      "Epoch 27/30, Train Loss: 2.8336, Val Loss: 2.4046\n",
      "Epoch 28/30, Train Loss: 2.8211, Val Loss: 2.4822\n",
      "Epoch 29/30, Train Loss: 2.8359, Val Loss: 2.3961\n",
      "Epoch 30/30, Train Loss: 2.8303, Val Loss: 2.3644\n",
      "Fold 2/12\n",
      "Epoch 1/30, Train Loss: 3.4500, Val Loss: 2.5122\n",
      "Epoch 2/30, Train Loss: 3.0930, Val Loss: 2.4578\n",
      "Epoch 3/30, Train Loss: 3.0340, Val Loss: 2.4267\n",
      "Epoch 4/30, Train Loss: 2.9956, Val Loss: 2.3850\n",
      "Epoch 5/30, Train Loss: 2.9716, Val Loss: 2.4063\n",
      "Epoch 6/30, Train Loss: 2.9395, Val Loss: 2.3648\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 29\u001B[0m\n\u001B[0;32m     26\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m     27\u001B[0m train_losses \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m---> 29\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m X_cat_batch, X_cont_batch, y_batch \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[0;32m     30\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     31\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m model(X_cat_batch, X_cont_batch)\n",
      "File \u001B[1;32mC:\\ANACONDA\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mC:\\ANACONDA\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1329\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[0;32m   1328\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m-> 1329\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_data()\n\u001B[0;32m   1330\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1331\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[0;32m   1332\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ANACONDA\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1295\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1291\u001B[0m     \u001B[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001B[39;00m\n\u001B[0;32m   1292\u001B[0m     \u001B[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001B[39;00m\n\u001B[0;32m   1293\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1294\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m-> 1295\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_get_data()\n\u001B[0;32m   1296\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[0;32m   1297\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[1;32mC:\\ANACONDA\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m   1120\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_try_get_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m_utils\u001B[38;5;241m.\u001B[39mMP_STATUS_CHECK_INTERVAL):\n\u001B[0;32m   1121\u001B[0m     \u001B[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001B[39;00m\n\u001B[0;32m   1122\u001B[0m     \u001B[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1130\u001B[0m     \u001B[38;5;66;03m# Returns a 2-tuple:\u001B[39;00m\n\u001B[0;32m   1131\u001B[0m     \u001B[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1133\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_queue\u001B[38;5;241m.\u001B[39mget(timeout\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[0;32m   1134\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n\u001B[0;32m   1135\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1136\u001B[0m         \u001B[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001B[39;00m\n\u001B[0;32m   1137\u001B[0m         \u001B[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001B[39;00m\n\u001B[0;32m   1138\u001B[0m         \u001B[38;5;66;03m# worker failures.\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ANACONDA\\Lib\\multiprocessing\\queues.py:113\u001B[0m, in \u001B[0;36mQueue.get\u001B[1;34m(self, block, timeout)\u001B[0m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m block:\n\u001B[0;32m    112\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m deadline \u001B[38;5;241m-\u001B[39m time\u001B[38;5;241m.\u001B[39mmonotonic()\n\u001B[1;32m--> 113\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_poll(timeout):\n\u001B[0;32m    114\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Empty\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_poll():\n",
      "File \u001B[1;32mC:\\ANACONDA\\Lib\\multiprocessing\\connection.py:257\u001B[0m, in \u001B[0;36m_ConnectionBase.poll\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_closed()\n\u001B[0;32m    256\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_readable()\n\u001B[1;32m--> 257\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_poll(timeout)\n",
      "File \u001B[1;32mC:\\ANACONDA\\Lib\\multiprocessing\\connection.py:346\u001B[0m, in \u001B[0;36mPipeConnection._poll\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_got_empty_message \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m    344\u001B[0m             _winapi\u001B[38;5;241m.\u001B[39mPeekNamedPipe(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle)[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m    345\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 346\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mbool\u001B[39m(wait([\u001B[38;5;28mself\u001B[39m], timeout))\n",
      "File \u001B[1;32mC:\\ANACONDA\\Lib\\multiprocessing\\connection.py:895\u001B[0m, in \u001B[0;36mwait\u001B[1;34m(object_list, timeout)\u001B[0m\n\u001B[0;32m    892\u001B[0m                 ready_objects\u001B[38;5;241m.\u001B[39madd(o)\n\u001B[0;32m    893\u001B[0m                 timeout \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m--> 895\u001B[0m     ready_handles \u001B[38;5;241m=\u001B[39m _exhaustive_wait(waithandle_to_obj\u001B[38;5;241m.\u001B[39mkeys(), timeout)\n\u001B[0;32m    896\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    897\u001B[0m     \u001B[38;5;66;03m# request that overlapped reads stop\u001B[39;00m\n\u001B[0;32m    898\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m ov \u001B[38;5;129;01min\u001B[39;00m ov_list:\n",
      "File \u001B[1;32mC:\\ANACONDA\\Lib\\multiprocessing\\connection.py:827\u001B[0m, in \u001B[0;36m_exhaustive_wait\u001B[1;34m(handles, timeout)\u001B[0m\n\u001B[0;32m    825\u001B[0m ready \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    826\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m L:\n\u001B[1;32m--> 827\u001B[0m     res \u001B[38;5;241m=\u001B[39m _winapi\u001B[38;5;241m.\u001B[39mWaitForMultipleObjects(L, \u001B[38;5;28;01mFalse\u001B[39;00m, timeout)\n\u001B[0;32m    828\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m res \u001B[38;5;241m==\u001B[39m WAIT_TIMEOUT:\n\u001B[0;32m    829\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "342c7642155b8614"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
