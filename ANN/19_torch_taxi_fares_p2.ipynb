{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-13T17:37:23.384804Z",
     "start_time": "2024-08-13T17:37:21.215022Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import copy\n",
    "# from class_models import early_stop"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T17:37:24.352580Z",
     "start_time": "2024-08-13T17:37:24.328841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# setting device and reproducibility\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42)\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "8b6358891ee8e788",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Learning schedule to find the best learning rate using *ReduceOnPlateau* scheduler\n",
    "The learning rate scheduler (ReduceLROnPlateau) adjusts the learning rate based on validation loss. The use of ReduceLROnPlateau scheduler helps in adjusting the learning rate dynamically based on the validation loss, which can improve the training process. \n",
    "\n",
    "- This training task tracks the best model in the k-fold. \n",
    "- The best model is tracked by comparing the validation loss of each epoch. \n",
    "- The learning rate (best_lr) that resulted in the lowest validation loss is recorded."
   ],
   "id": "e47ae840dd103484"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T17:37:27.530562Z",
     "start_time": "2024-08-13T17:37:27.423764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import data \n",
    "data_frame = pd.read_csv(\"data/NYCTaxiFares.csv\", na_values=[\"NA\", \"?\"])"
   ],
   "id": "5aa4b73fe3e92c46",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T17:37:29.700591Z",
     "start_time": "2024-08-13T17:37:29.692712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function to calculate the distance of the travel\n",
    "def haversine_distance(dat_f, lat1, lon1, lat2, lon2):\n",
    "    \n",
    "    # average radius of the Earth in (km)\n",
    "    r = 6371\n",
    "    \n",
    "    phi1 = np.radians(dat_f[lat1])\n",
    "    phi2 = np.radians(dat_f[lat2])\n",
    "    delta_phi = np.radians(dat_f[lat2] - dat_f[lat1])\n",
    "    delta_lambda = np.radians(dat_f[lon2] - dat_f[lon1])\n",
    "    \n",
    "    a = np.sin(delta_phi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    d = (r * c)\n",
    "    \n",
    "    return d\n",
    "\n",
    "def preprocessing(df_n, cat_cols):\n",
    "    \"\"\"\n",
    "    Preprocesses the data and adds pandas categorical fields to a dataframe.\n",
    "    :param df_n: pandas dataframe \n",
    "    :param cat_cols: list of categorical fields\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    # append a 'dist_km' new feature in the dataframe\n",
    "    df_n['dist_km'] = haversine_distance(df_n, 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude')\n",
    "    \n",
    "    # remove outliers\n",
    "    dfd = df_n[(df_n['fare_amount'] != 49.57) & (df_n['fare_amount'] != 45.00)].copy()\n",
    "    \n",
    "    # convert to pd datetime\n",
    "    dfd['pickup_datetime'] = pd.to_datetime(dfd['pickup_datetime'])\n",
    "    \n",
    "    # Correcting pickup_datetime due to daylight savings time (April)\n",
    "    dfd['EDTdate'] = dfd['pickup_datetime'] - pd.Timedelta(hours=4)\n",
    "    \n",
    "    # create new time fields\n",
    "    dfd['Hour'] = dfd['EDTdate'].dt.hour\n",
    "    dfd['AMorPM'] = np.where(dfd['Hour']<12, 'am', 'pm')\n",
    "    dfd['Weekday'] = dfd['EDTdate'].dt.strftime(\"%a\")\n",
    "    \n",
    "    # transform to pandas categorical variables\n",
    "    for cat in cat_cols:\n",
    "        dfd[cat] = dfd[cat].astype('category')\n",
    "    \n",
    "    dfd = dfd.drop(columns=['pickup_datetime'])\n",
    "    \n",
    "    return dfd\n",
    "\n",
    "def model_tensors(df, cat_cols, cont_cols, y_col):\n",
    "    \"\"\"\n",
    "    Get categorical, continuous and label tensors for the model\n",
    "    :param df: pd dataframe\n",
    "    :param cat_cols: list of categorical fields\n",
    "    :param cont_cols: list of continuous fields\n",
    "    :param y_col: list with the labels\n",
    "    :return: cats, conts, y tensors\n",
    "    \"\"\"\n",
    "    \n",
    "    # group the data in categorical continuous and target label    \n",
    "    cats = np.stack([df[col].cat.codes.values for col in cat_cols], axis=1)\n",
    "    conts = np.stack([df[col].values for col in cont_cols], axis=1)\n",
    "    y = df[y_col].values.reshape(-1, 1)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    cats_t = torch.tensor(cats, dtype=torch.int64).to(device)\n",
    "    conts_t = torch.tensor(conts, dtype=torch.float32).to(device)\n",
    "    y_t = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "    \n",
    "    return cats_t, conts_t, y_t\n",
    "\n",
    "def create_embedding_sizes(df, cat_cols):\n",
    "    \"\"\"\n",
    "    Create embedding sizes for PyTorch embedding layers\n",
    "    :param df: pandas dataframe\n",
    "    :param cat_cols: list of categorical fields\n",
    "    :return: emb_sizes list\n",
    "    \"\"\"\n",
    "    # categorical sizes list\n",
    "    cat_sizes = [len(df[col].cat.categories) for col in cat_cols]\n",
    "\n",
    "    # embedding sizes list (divide the number of unique entries in each column by two, if the result is greater than 50 select 50)\n",
    "    emb_sizes = [(size, min(50,(size+1)//2)) for size in cat_sizes]\n",
    "    \n",
    "    return emb_sizes\n",
    "\n"
   ],
   "id": "9b52fe568f07205e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T17:37:32.877726Z",
     "start_time": "2024-08-13T17:37:31.379779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = preprocessing(data_frame, ['Hour', 'AMorPM', 'Weekday'])\n",
    "\n",
    "cats, conts, y = model_tensors(df, ['Hour', 'AMorPM', 'Weekday'], ['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude', 'dist_km'], ['fare_amount'])\n",
    "\n",
    "# number of continuous fields of the conts tensor\n",
    "n_cont = conts.shape[1]\n",
    "\n",
    "emb_sizes = create_embedding_sizes(df, ['Hour', 'AMorPM', 'Weekday'])"
   ],
   "id": "6415281a4de6470",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model definition",
   "id": "fe20a455c5e22d44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the model\n",
    "class TabularModel(nn.Module):\n",
    "    def __init__(self, emb_sizes, n_cont, out_size, layers, p=0.5):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in emb_sizes])\n",
    "        self.emb_drop = nn.Dropout(p)\n",
    "        self.batch_norm_cont = nn.BatchNorm1d(n_cont)\n",
    "\n",
    "        layer_list = []\n",
    "        n_emb = sum([nf for ni, nf in emb_sizes])\n",
    "        n_in = n_emb + n_cont\n",
    "        for i in layers:\n",
    "            layer_list.append(nn.Linear(n_in, i))\n",
    "            layer_list.append(nn.ReLU(inplace=True))\n",
    "            layer_list.append(nn.BatchNorm1d(i))\n",
    "            layer_list.append(nn.Dropout(p))\n",
    "            n_in = i\n",
    "\n",
    "        layer_list.append(nn.Linear(layers[-1], out_size))\n",
    "        self.layers = nn.Sequential(*layer_list)\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        embeddings = [e(x_cat[:, i]) for i, e in enumerate(self.embeds)]\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        x_cont = self.batch_norm_cont(x_cont)\n",
    "        x = torch.cat([x, x_cont], 1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ],
   "id": "e7bc0aa7a2948d56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### K-folds function generator\n",
    "Since the *train_index* and the *val_index* are those who effectively creates the training subsets we can create all the six subsets: 2 for categorical, 2 for continuous and 2 for y labels."
   ],
   "id": "716c3d6415d5e47b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function-generator to split the data into k folds\n",
    "def kfold_split(k, X_cat, X_cont, y):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    for train_index, val_index in kf.split(X_cat):\n",
    "        X_cat_train, X_cat_val = X_cat[train_index], X_cat[val_index]\n",
    "        X_cont_train, X_cont_val = X_cont[train_index], X_cont[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        yield X_cat_train, X_cat_val, X_cont_train, X_cont_val, y_train, y_val"
   ],
   "id": "78fbc603770e3584",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model and training parameters",
   "id": "3e7f5652ef3b64d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# emb_sizes\n",
    "# cont r\n",
    "out_size = 1 # <- for regression task\n",
    "layers = [200, 100]\n",
    "p = 0.2 # dropout probability\n",
    "batch_size = 1024\n",
    "epochs = 100"
   ],
   "id": "a6ac861eeaadc06e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cross-Validation loop",
   "id": "fcdb5be4d2efddbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cross-validation loop\n",
    "k = 12\n",
    "fold = 0\n",
    "best_lr = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "for X_cat_train, X_cat_val, X_cont_train, X_cont_val, y_train, y_val in kfold_split(k, cats, conts, y):\n",
    "    fold += 1\n",
    "    print(f\"Fold {fold}/{k}\")\n",
    "\n",
    "    # Create datasets and loaders\n",
    "    train_dataset = TensorDataset(X_cat_train, X_cont_train, y_train)\n",
    "    val_dataset = TensorDataset(X_cat_val, X_cont_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize model, loss, optimizer, and scheduler\n",
    "    model = TabularModel(emb_sizes, n_cont, out_size, layers, p).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        for X_cat_batch, X_cont_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_cat_batch, X_cont_batch)\n",
    "            loss = torch.sqrt(criterion(y_pred, y_batch))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for X_cat_batch, X_cont_batch, y_batch in val_loader:\n",
    "                y_pred = model(X_cat_batch, X_cont_batch)\n",
    "                loss = torch.sqrt(criterion(y_pred, y_batch))\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        mean_val_loss = np.mean(val_losses)\n",
    "        scheduler.step(mean_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {np.mean(train_losses):.4f}, Val Loss: {mean_val_loss:.4f}\")\n",
    "\n",
    "        # Check for best model\n",
    "        if mean_val_loss < best_loss:\n",
    "            best_loss = mean_val_loss\n",
    "            best_lr = optimizer.param_groups[0]['lr']\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "print(f\"Best Learning Rate: {best_lr}\")"
   ],
   "id": "19d52c6a0d1d495b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to reset weights and biases\n",
    "def reset_weights(m):\n",
    "    if hasattr(m, 'reset_parameters'):\n",
    "        m.reset_parameters()\n",
    "\n",
    "# Apply the reset function to each layer\n",
    "model.apply(reset_weights)"
   ],
   "id": "4c0afe8311e852c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Learning schedule changing the network architecture, the drop-off probability and applying adaptively learning rate",
   "id": "61ab1e4870a154c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# emb_sizes\n",
    "# cont\n",
    "out_size = 1\n",
    "\n",
    "# Experiment with different architectures and dropout rates\n",
    "arch_configs = [\n",
    "    ([50, 25], 0.1),\n",
    "    ([50, 25], 0.2),\n",
    "    ([50, 25], 0.3),\n",
    "    ([50, 25], 0.4),\n",
    "    ([100, 50], 0.1),\n",
    "    ([100, 50], 0.2),\n",
    "    ([100, 50], 0.3),\n",
    "    ([100, 50], 0.4),\n",
    "    ([200, 100], 0.1),\n",
    "    ([200, 100], 0.2),\n",
    "    ([200, 100], 0.3),\n",
    "    ([200, 100], 0.4),\n",
    "    ([200, 100, 50], 0.1),\n",
    "    ([200, 100, 50], 0.2),\n",
    "    ([200, 100, 50], 0.3),\n",
    "    ([200, 100, 50], 0.4),\n",
    "]\n",
    "\n",
    "arch_configs_r = arch_configs[::-1] # <- inverse the architecture configurations list \n",
    "\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "k = 12"
   ],
   "id": "88e9faaf8e9e7724",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for arch, p in arch_configs_r:\n",
    "    fold = 0\n",
    "    best_lr = None\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    print(f\"Testing architecture: {arch} with dropout: {p}\")\n",
    "\n",
    "    for X_cat_train, X_cat_val, X_cont_train, X_cont_val, y_train, y_val in kfold_split(k, cats, conts, y):\n",
    "        fold += 1\n",
    "        print(f\"Fold {fold}/{k}\")\n",
    "\n",
    "        # Create datasets and loaders\n",
    "        train_dataset = TensorDataset(X_cat_train, X_cont_train, y_train)\n",
    "        val_dataset = TensorDataset(X_cat_val, X_cont_val, y_val)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers = 2)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers = 2)\n",
    "\n",
    "        # Initialize model, loss, optimizer, and scheduler\n",
    "        model = TabularModel(emb_sizes, n_cont, out_size, arch, p).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            train_losses = []\n",
    "\n",
    "            for X_cat_batch, X_cont_batch, y_batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(X_cat_batch, X_cont_batch)\n",
    "                loss = torch.sqrt(criterion(y_pred, y_batch))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            with torch.no_grad():\n",
    "                for X_cat_batch, X_cont_batch, y_batch in val_loader:\n",
    "                    y_pred = model(X_cat_batch, X_cont_batch)\n",
    "                    loss = torch.sqrt(criterion(y_pred, y_batch))\n",
    "                    val_losses.append(loss.item())\n",
    "\n",
    "            mean_val_loss = np.mean(val_losses)\n",
    "            scheduler.step(mean_val_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {np.mean(train_losses):.4f}, Val Loss: {mean_val_loss:.4f}\")\n",
    "\n",
    "            # Check for best model\n",
    "            if mean_val_loss < best_loss:\n",
    "                best_loss = mean_val_loss\n",
    "                best_lr = optimizer.param_groups[0]['lr']\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # Save results for the current fold and configuration\n",
    "        results.append({\n",
    "            \"Architecture\": str(arch),\n",
    "            \"Dropout\": p,\n",
    "            \"Fold\": fold,\n",
    "            \"Best Learning Rate\": best_lr,\n",
    "            \"Best Validation Loss\": best_loss\n",
    "        })"
   ],
   "id": "5cc0cd1c9a1966aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "342c7642155b8614",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
