{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    A Minimalist Computation Graph Framework \n",
    "https://github.com/davidrosenberg/mlcourse/blob/gh-pages/Notebooks/computation-graph/computation-graph-framework.ipynb"
   ],
   "id": "aa3edf2027219f10"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T04:04:52.402874Z",
     "start_time": "2024-05-09T04:04:52.314831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from IPython.display import Image"
   ],
   "id": "add648b5a82620e8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We start with a completely trival example by making a computation graph with a single node,  representing a scalar value $x$. To represent $x$, we'll use a node called a \"value node\", since it takes no input, but its output is the value of $x$.  Later, in machine learning contexts, we'll use nodes of this type to represent inputs, outcomes, and parameters.\n",
    "\n",
    "We will interpret this single-node computation graph as representing the function $f(x)=x$.  Thus the \"graph output\", i.e the scalar-valued output of the function our computation graph represents, is also the scalar $x$.  We will refer to the node whose output is the graph output as the \"graph output node\".  Our computation graphs are only designed to represent scalar-valued functions, since our goal is to find inputs that minimize of maximize the function, and it's not clear what that means for a vector-valued function.\n",
    "\n",
    "We will represent value nodes by instances of the class `ValueNode`, defined below:"
   ],
   "id": "423fc3f1c9dd1b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T04:04:55.394916Z",
     "start_time": "2024-05-09T04:04:55.391915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ValueNode(object):\n",
    "    \"\"\"Computation graph node having no input but simply holding a value\"\"\"\n",
    "    def __init__(self, node_name):\n",
    "        self.node_name = node_name\n",
    "        self.out = None\n",
    "        self.d_out = None\n",
    "    \n",
    "    def forward(self):\n",
    "        self.d_out = np.zeros(self.out.shape)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        return self.d_out\n",
    "    \n",
    "    def get_predecessors(self):\n",
    "        return []\n",
    "    "
   ],
   "id": "35e3c174ed38cfc1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To give a `ValueNode` a particular output value, we directly set it.  It should always be a numpy array. For example, for a scalar value we can set it as follows:",
   "id": "926d7e1834591fbe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T04:05:00.317915Z",
     "start_time": "2024-05-09T04:05:00.315400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = ValueNode(\"x\")\n",
    "x.out = np.array(3)"
   ],
   "id": "8cfde1fa6d1689d9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In backpropagation, we compute the partial derivative of the graph output with respect to the outputs of every node in the graph. Every node in the graph contains an instance variable called out, witch is a numpy array representing the output of that node.\n",
    "\n",
    "Every node also has an instance variable d_out, which during backpropagation is set to contain the partial derivatives of the graph output with respect to the entries of out of the same node. In general, a variable named d_X wil be used tO represent the derivative of the graph output with respect to X. So ir we use J to represent the sacalar-valued graph output as we commonly do, then d_out, d_x, and d_y should be undenstood to represent $\\frac{\\partial J}{\\partial \\text{out}}$, $\\frac{\\partial J}{\\partial \\text{x}}$, and $\\frac{\\partial J}{\\partial \\text{y}}$, respectively. \n",
    "\n",
    " In any node, `d_out` will always be a `numpy` array of the same shape as `out`.\n",
    "\n",
    "Backpropagation always starts by setting `d_out` for the graph output node to the scalar `1`, since it is the partial derivative of the graph output with repect to itself.\n",
    "\n",
    "Let's do that for the node in our trivial example.  Note that for a `ValueNode`, the `backward` function just returns `d_out`."
   ],
   "id": "632b6a2f850874ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d950e9c5cacfffe8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T04:05:05.230094Z",
     "start_time": "2024-05-09T04:05:05.226094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = ValueNode(\"x\")\n",
    "x.out = np.array(3) # set the value of x\n",
    "print(x.forward()) # both the node output and the graph output\n",
    "x.d_out = np.array(1) # initialize backpropagation\n",
    "print(x.backward()) # the partial derivative of the graph output w.r.t. the output of x"
   ],
   "id": "db58fbd71641dc96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "89c899714af8a937"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
