{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T01:44:44.932473Z",
     "start_time": "2024-06-03T01:44:44.799101Z"
    }
   },
   "cell_type": "code",
   "source": "import numpy as np",
   "id": "7ccbcfc0eebf96ad",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Backpropagation with learning algorithm",
   "id": "4e876bc45d13058f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The backpropagation algorithm computes the gradient of the cost function for a single training example, $C = C_x$. In practice, it's common to combine backpropagation with a learning algorithm such as 'stochastic gradient descent', in which we compute the gradient for many training examples. In particular, given  a mini-batch of $m$ training examples, the following algorithm applies a gradient descent learning step based on a 'mini-batch':\n",
    "\n",
    "1. Input a set of training examples\n",
    "2. For each training example x: Set the corresponding input activation $a^{x,1}$, and perform the following steps:\n",
    "   - Feedforward: For each $l = 2, 3, \\ldots, L$ compute $z^{x,l} = w^l a^{x,l-1}+b^l$ and $a^{x,l} = \\sigma(z^{x,l})$\n",
    "   - Output error $\\delta^{x,L}$: Compute the vector $\\delta^{x,L} = \\nabla_a C_x \\odot \\sigma'(z^{x,L})$\n",
    "   - Backpropagate the error: For each $l = L-1, L-2,\n",
    "  \\ldots, 2$ compute $\\delta^{x,l} = ((w^{l+1})^T \\delta^{x,l+1})\n",
    "  \\odot \\sigma'(z^{x,l})$\n",
    "3. Gradient descent: For each $l = L, L-1, \\ldots, 2$ update the weights according to the rule $w^l \\rightarrow\n",
    "  w^l-\\frac{\\eta}{m} \\sum_x \\delta^{x,l} (a^{x,l-1})^T$, and the biases according to the rule $b^l \\rightarrow b^l-\\frac{\\eta}{m}\n",
    "  \\sum_x \\delta^{x,l}$"
   ],
   "id": "7105b9e09454c84b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating a network class for training a neural network with backpropagation and stochastic gradient descent ",
   "id": "c9a0f3bf3d1a2914"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T02:47:58.260903Z",
     "start_time": "2024-06-03T02:41:43.942023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        parameters\n",
    "        sizes: a LIST containing the number of neurons in the respective layers \n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "        \n",
    "net = Network([4,5,2])\n",
    "        "
   ],
   "id": "a88f97d01bfb568d",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T02:32:51.339982Z",
     "start_time": "2024-06-03T02:32:51.336104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"The biases of the hidden layer \", net.biases[0])\n",
    "print(\"The biases of the output layer \", net.biases[1])"
   ],
   "id": "6fb68ed218b8a3a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biases of the first layer  [[ 1.15494994]\n",
      " [-0.6007814 ]\n",
      " [-0.24239692]\n",
      " [ 1.78980074]\n",
      " [-0.3145079 ]]\n",
      "The biases of the output layer  [[-0.74048213]\n",
      " [-0.71329663]]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T02:33:05.410439Z",
     "start_time": "2024-06-03T02:33:05.407518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"The weights connecting the input layer with the hidden layer \", net.weights[0])\n",
    "print(\"The weights connecting the first layer with the output layer  \", net.weights[1])"
   ],
   "id": "f05cc9d59844da0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights connecting the input layer with the first layer  [[-0.5429775  -0.02178585  0.72909077  0.46251342]\n",
      " [ 1.24434343  0.73691073  2.19286778  0.71802016]\n",
      " [-0.13738598  1.67472041  0.19574636 -0.01059091]\n",
      " [-0.67570365  0.84736245 -1.80284305 -2.30078438]\n",
      " [ 0.76816213  1.85204346 -0.04290432  1.34077169]]\n",
      "The weights connecting the first layer with the output layer   [[ 0.22725836 -0.22472271 -1.18352648  0.53570675  1.29350688]\n",
      " [-0.61104483 -0.52933924  0.04232861 -0.19318958  1.28618062]]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2ef451a96b1989aa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
